{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0V4W0-bNG2f",
        "outputId": "0853d545-9ced-4305-f6aa-98ace436f727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python efficientad.py --dataset mvtec_ad --subdataset wood --train_steps 6000\n",
        "!python '/content/drive/MyDrive/EfficientAD_/efficientad.py' --mvtec_ad_path \"/content/drive/MyDrive/wood_otsu\" --subdataset \"wood\" --train_steps 1500 -o \"/content/drive/MyDrive/EfficientAD_/Deneme 5 AgÌ†Ä±rlÄ±klar\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-vtJnkMQyxm",
        "outputId": "2a690a38-9f24-48b1-e44a-44888a93df4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing mean of features: 100% 63/63 [00:00<00:00, 138.61it/s]\n",
            "Computing std of features: 100% 63/63 [00:00<00:00, 209.63it/s]\n",
            "Current loss: 8.7328: 100% 1500/1500 [00:24<00:00, 61.60it/s]\n",
            "Final map normalization: 100% 7/7 [00:00<00:00, 106.96it/s]\n",
            "Final inference: 100% 140/140 [02:26<00:00,  1.05s/it]\n",
            "\n",
            "ðŸ” IoU Threshold Optimization Results:\n",
            "Threshold: 0.10 | Mean IoU: 0.2724\n",
            "Threshold: 0.20 | Mean IoU: 0.1989\n",
            "Threshold: 0.30 | Mean IoU: 0.1219\n",
            "Threshold: 0.40 | Mean IoU: 0.0626\n",
            "Threshold: 0.50 | Mean IoU: 0.0306\n",
            "Threshold: 0.60 | Mean IoU: 0.0134\n",
            "Threshold: 0.70 | Mean IoU: 0.0065\n",
            "Threshold: 0.80 | Mean IoU: 0.0025\n",
            "Threshold: 0.90 | Mean IoU: 0.0010\n",
            "\n",
            "âœ… Best IoU: 0.2724 @ Threshold = 0.10\n",
            "\n",
            "âœ… Anomaly Detection SonuclarÄ±\n",
            "AUC Score          : 0.9171\n",
            "F1 Score           : 0.8904\n",
            "Precision          : 0.8553\n",
            "Recall             : 0.9286\n",
            "Optimal Threshold  : 0.1834\n",
            "Confusion Matrix:\n",
            " [[59 11]\n",
            " [ 5 65]]\n",
            "\n",
            "âœ… Final Anomaly Detection SonuÃ§larÄ±\n",
            "Final AUC Score        : 0.9171\n",
            "Final F1 Score         : 0.8904\n",
            "Final Precision        : 0.8553\n",
            "Final Recall           : 0.9286\n",
            "Final Confusion Matrix :\n",
            " [[59 11]\n",
            " [ 5 65]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Save"
      ],
      "metadata": {
        "id": "k-C_Z7xAa9uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# GÃ¼venli sÄ±nÄ±flar (PyTorch 2.6+ iÃ§in)\n",
        "add_safe_globals([\n",
        "    torch.nn.Sequential,\n",
        "    torch.nn.Conv2d,\n",
        "    torch.nn.ReLU,\n",
        "    torch.nn.Linear,\n",
        "    torch.nn.BatchNorm2d,\n",
        "    torch.nn.AvgPool2d,\n",
        "    torch.nn.MaxPool2d,\n",
        "    torch.nn.Flatten,\n",
        "    transforms.Compose\n",
        "])\n",
        "\n",
        "# Yol ayarlarÄ±\n",
        "MODEL_SAVE_DIR = \"/content/drive/MyDrive/EfficientAD_/Deneme 5 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\"\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"\n",
        "save_path = os.path.join(MODEL_SAVE_DIR, \"efficientad_complete_model.pth\")\n",
        "\n",
        "# Transform\n",
        "image_size = 256\n",
        "out_channels = 384\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Model ve quantile yÃ¼kle\n",
        "teacher = torch.load(os.path.join(MODEL_SAVE_DIR, 'teacher_final.pth'), weights_only=False)\n",
        "student = torch.load(os.path.join(MODEL_SAVE_DIR, 'student_final.pth'), weights_only=False)\n",
        "autoencoder = torch.load(os.path.join(MODEL_SAVE_DIR, 'autoencoder_final.pth'), weights_only=False)\n",
        "quantiles = torch.load(os.path.join(MODEL_SAVE_DIR, 'map_quantiles_final.pth'), weights_only=False)\n",
        "\n",
        "# âœ… float / numpy / tensor ayÄ±rt ederek tensor'a Ã§evir\n",
        "def ensure_tensor(v):\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        return v\n",
        "    elif isinstance(v, np.ndarray):\n",
        "        return torch.from_numpy(v)\n",
        "    elif isinstance(v, (float, int)):\n",
        "        return torch.tensor(v)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported type in quantiles: {type(v)}\")\n",
        "\n",
        "map_quantiles = {k: ensure_tensor(v) for k, v in quantiles.items()}\n",
        "\n",
        "# Normalizasyon hesapla\n",
        "train_images = []\n",
        "for ext in [\"*.jpg\", \"*.png\", \"**/*.jpg\", \"**/*.png\", \"good/*.jpg\", \"good/*.png\"]:\n",
        "    train_images.extend(glob(os.path.join(TRAIN_DIR, ext), recursive=True))\n",
        "\n",
        "if not train_images:\n",
        "    print(\"No training images found. Using dummy stats.\")\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "else:\n",
        "    teacher_cpu = teacher.cpu()\n",
        "    mean_outputs = []\n",
        "    for img_path in tqdm(train_images[:50], desc=\"Computing mean\"):\n",
        "        try:\n",
        "            img = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0)\n",
        "            out = teacher_cpu(img)\n",
        "            mean_outputs.append(out.mean(dim=[0, 2, 3]))\n",
        "        except Exception as e:\n",
        "            print(f\"Mean error {img_path}: {e}\")\n",
        "    channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)[None, :, None, None]\n",
        "\n",
        "    std_outputs = []\n",
        "    for img_path in tqdm(train_images[:50], desc=\"Computing std\"):\n",
        "        try:\n",
        "            img = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0)\n",
        "            out = teacher_cpu(img)\n",
        "            std_outputs.append(((out - channel_mean)**2).mean(dim=[0, 2, 3]))\n",
        "        except Exception as e:\n",
        "            print(f\"Std error {img_path}: {e}\")\n",
        "    channel_var = torch.mean(torch.stack(std_outputs), dim=0)[None, :, None, None]\n",
        "    channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "# âž¤ Tensor olarak kaydedilecek stat dict\n",
        "normalization_params = {\n",
        "    'mean': channel_mean,\n",
        "    'std': channel_std\n",
        "}\n",
        "\n",
        "# Modeli kaydet\n",
        "torch.save({\n",
        "    \"teacher\": teacher.cpu(),\n",
        "    \"student\": student.cpu(),\n",
        "    \"autoencoder\": autoencoder.cpu(),\n",
        "    \"map_quantiles\": map_quantiles,\n",
        "    \"normalization_params\": normalization_params\n",
        "}, save_path)\n",
        "\n",
        "print(\"âœ… efficientad_complete_model.pth baÅŸarÄ±yla ve doÄŸru formatta kaydedildi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg1UMwPHbEFG",
        "outputId": "0799f798-8338-432f-a80a-28e1064d1551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No training images found. Using dummy stats.\n",
            "âœ… efficientad_complete_model.pth baÅŸarÄ±yla ve doÄŸru formatta kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelin Ä°Ã§eriÄŸi"
      ],
      "metadata": {
        "id": "3YWnJVKNfAZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle  # Bu Ã¶nemli\n",
        "\n",
        "path = \"/content/drive/MyDrive/EfficientAD_/Deneme 5 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood/efficientad_complete_model.pth\"\n",
        "\n",
        "# pickle_module=pickle ile yÃ¼klemeyi dene\n",
        "data = torch.load(path, map_location=\"cpu\", pickle_module=pickle)\n",
        "\n",
        "print(\"Anahtarlar:\", data.keys())  # dict_keys([...])\n",
        "\n",
        "for key, value in data.items():\n",
        "    print(f\"\\nðŸ”‘ {key} --> {type(value)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqqZudupfDxQ",
        "outputId": "bc43ad5e-ee2e-4310-8317-e821d7518bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anahtarlar: dict_keys(['teacher', 'student', 'autoencoder', 'map_quantiles', 'normalization_params'])\n",
            "\n",
            "ðŸ”‘ teacher --> <class 'torch.nn.modules.container.Sequential'>\n",
            "\n",
            "ðŸ”‘ student --> <class 'torch.nn.modules.container.Sequential'>\n",
            "\n",
            "ðŸ”‘ autoencoder --> <class 'torch.nn.modules.container.Sequential'>\n",
            "\n",
            "ðŸ”‘ map_quantiles --> <class 'dict'>\n",
            "\n",
            "ðŸ”‘ normalization_params --> <class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DevamÄ±"
      ],
      "metadata": {
        "id": "5IkLl6s7bBYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import tifffile\n",
        "\n",
        "# Fix for PyTorch 2.6+ pickle loading issue\n",
        "from torch.serialization import add_safe_globals\n",
        "add_safe_globals([\n",
        "    'torch.nn.modules.container.Sequential',\n",
        "    'torch.nn.modules.conv.Conv2d',\n",
        "    'torch.nn.modules.activation.ReLU',\n",
        "    'torch.nn.modules.pooling.AvgPool2d',\n",
        "    'torch.nn.modules.linear.Linear',\n",
        "    'torch.nn.modules.normalization.GroupNorm',\n",
        "    'torch.nn.modules.activation.GELU',\n",
        "    'torchvision.models.resnet.Bottleneck',\n",
        "    'collections.OrderedDict'\n",
        "])\n",
        "\n",
        "# Paths - UPDATE THESE TO MATCH YOUR ACTUAL DIRECTORIES\n",
        "DATASET_PATH = \"/content/drive/MyDrive/wood_otsu/wood/ground_truth\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/EfficientAD_/result-2/trainings/mvtec_ad/wood\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/EfficientAD_/result-ground_truth/png\"\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"  # Path to training images\n",
        "\n",
        "# Print paths for verification\n",
        "print(f\"Dataset Path: {DATASET_PATH}\")\n",
        "print(f\"Model Path: {MODEL_PATH}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"Train Directory: {TRAIN_DIR}\")\n",
        "\n",
        "# Check if directories exist\n",
        "for path, name in [(DATASET_PATH, \"Dataset\"), (MODEL_PATH, \"Model\"), (TRAIN_DIR, \"Train\")]:\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"WARNING: {name} path does not exist: {path}\")\n",
        "    else:\n",
        "        print(f\"{name} path exists: {path}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "on_gpu = torch.cuda.is_available()\n",
        "out_channels = 384\n",
        "image_size = 256\n",
        "\n",
        "# Default transform for testing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Loading models and quantiles...\")\n",
        "try:\n",
        "    # Try loading with weights_only=False to handle PyTorch 2.6 changes\n",
        "    teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_final.pth'), weights_only=False)\n",
        "    student = torch.load(os.path.join(MODEL_PATH, 'student_final.pth'), weights_only=False)\n",
        "    autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_final.pth'), weights_only=False)\n",
        "    print(\"Successfully loaded final models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading final models: {e}\")\n",
        "    try:\n",
        "        # Try temporary models\n",
        "        print(\"Trying temporary models...\")\n",
        "        teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_tmp.pth'), weights_only=False)\n",
        "        student = torch.load(os.path.join(MODEL_PATH, 'student_tmp.pth'), weights_only=False)\n",
        "        autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_tmp.pth'), weights_only=False)\n",
        "        print(\"Successfully loaded temporary models\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporary models: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Load quantiles\n",
        "print(\"Loading quantiles...\")\n",
        "try:\n",
        "    quantiles = torch.load(os.path.join(MODEL_PATH, 'map_quantiles_final.pth'), weights_only=False)\n",
        "    q_st_start = torch.tensor(quantiles['q_st_start']) # Convert to tensor\n",
        "    q_st_end = torch.tensor(quantiles['q_st_end'])  # Convert to tensor\n",
        "    q_ae_start = torch.tensor(quantiles['q_ae_start'])  # Convert to tensor\n",
        "    q_ae_end = torch.tensor(quantiles['q_ae_end'])  # Convert to tensor\n",
        "    print(\"Successfully loaded quantiles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantiles: {e}\")\n",
        "    raise\n",
        "\n",
        "if on_gpu:\n",
        "    teacher = teacher.cuda()\n",
        "    student = student.cuda()\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    q_st_start = q_st_start.cuda()\n",
        "    q_st_end = q_st_end.cuda()\n",
        "    q_ae_start = q_ae_start.cuda()\n",
        "    q_ae_end = q_ae_end.cuda()\n",
        "\n",
        "# Set models to evaluation mode\n",
        "teacher.eval()\n",
        "student.eval()\n",
        "autoencoder.eval()\n",
        "\n",
        "# Load train data for computing teacher normalization\n",
        "print(\"Loading training images for normalization...\")\n",
        "\n",
        "# Try different patterns to locate training images\n",
        "train_patterns = [\n",
        "    os.path.join(TRAIN_DIR, \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.png\")\n",
        "]\n",
        "\n",
        "train_images = []\n",
        "for pattern in train_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} images with pattern: {pattern}\")\n",
        "        train_images.extend(found_images)\n",
        "\n",
        "if not train_images:\n",
        "    print(\"ERROR: No training images found! Checking parent directories...\")\n",
        "    # Try to look in parent directories\n",
        "    parent_dir = os.path.dirname(TRAIN_DIR)\n",
        "    for pattern in [os.path.join(parent_dir, \"**\", \"*.jpg\"), os.path.join(parent_dir, \"**\", \"*.png\")]:\n",
        "        found_images = glob(pattern, recursive=True)\n",
        "        if found_images:\n",
        "            print(f\"Found {len(found_images)} images in parent directory with pattern: {pattern}\")\n",
        "            train_images.extend(found_images)\n",
        "\n",
        "# Print first few train images for verification\n",
        "if train_images:\n",
        "    print(f\"Total training images found: {len(train_images)}\")\n",
        "    print(\"First few training images:\")\n",
        "    for i in range(min(5, len(train_images))):\n",
        "        print(f\"  - {train_images[i]}\")\n",
        "else:\n",
        "    print(\"ERROR: Could not find any training images! Cannot proceed without training images.\")\n",
        "    # Instead of crashing, we'll use pre-computed statistics from the quantiles\n",
        "    print(\"Using pre-computed statistics from quantiles to continue...\")\n",
        "    # Create some dummy statistics that should work reasonably well\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "    if on_gpu:\n",
        "        channel_mean = channel_mean.cuda()\n",
        "        channel_std = channel_std.cuda()\n",
        "\n",
        "# If we have training images, compute normalization statistics\n",
        "if train_images:\n",
        "    # If there are too many images, use just a subset for normalization\n",
        "    if len(train_images) > 50:\n",
        "        import random\n",
        "        train_images = random.sample(train_images, 50)\n",
        "        print(f\"Using random subset of 50 images for normalization\")\n",
        "\n",
        "    # Compute teacher normalization\n",
        "    print(\"Computing teacher normalization...\")\n",
        "    mean_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for img_path in tqdm(train_images):\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                image = transform(image).unsqueeze(0)\n",
        "                if on_gpu:\n",
        "                    image = image.cuda()\n",
        "                teacher_output = teacher(image)\n",
        "                mean_output = torch.mean(teacher_output, dim=[0, 2, 3])\n",
        "                mean_outputs.append(mean_output)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing training image {img_path}: {e}\")\n",
        "\n",
        "    if not mean_outputs:\n",
        "        print(\"ERROR: Failed to process any training images!\")\n",
        "        # Use dummy statistics\n",
        "        channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "        channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "        if on_gpu:\n",
        "            channel_mean = channel_mean.cuda()\n",
        "            channel_std = channel_std.cuda()\n",
        "    else:\n",
        "        channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)\n",
        "        channel_mean = channel_mean[None, :, None, None]\n",
        "\n",
        "        mean_distances = []\n",
        "        with torch.no_grad():\n",
        "            for img_path in tqdm(train_images):\n",
        "                try:\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image = transform(image).unsqueeze(0)\n",
        "                    if on_gpu:\n",
        "                        image = image.cuda()\n",
        "                    teacher_output = teacher(image)\n",
        "                    distance = (teacher_output - channel_mean) ** 2\n",
        "                    mean_distance = torch.mean(distance, dim=[0, 2, 3])\n",
        "                    mean_distances.append(mean_distance)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing training image distance {img_path}: {e}\")\n",
        "\n",
        "        channel_var = torch.mean(torch.stack(mean_distances), dim=0)\n",
        "        channel_var = channel_var[None, :, None, None]\n",
        "        channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "# Find all test images (both good and defect)\n",
        "print(\"Looking for test images...\")\n",
        "test_patterns = [\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.png\")\n",
        "]\n",
        "\n",
        "test_images = []\n",
        "for pattern in test_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} test images with pattern: {pattern}\")\n",
        "        test_images.extend(found_images)\n",
        "\n",
        "# Remove duplicates\n",
        "test_images = list(set(test_images))\n",
        "\n",
        "if not test_images:\n",
        "    print(\"ERROR: No test images found!\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Found {len(test_images)} total test images.\")\n",
        "print(\"First few test images:\")\n",
        "for i in range(min(5, len(test_images))):\n",
        "    print(f\"  - {test_images[i]}\")\n",
        "\n",
        "# Run inference on all test images\n",
        "print(\"Starting inference...\")\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(test_images, desc=\"Processing test images\"):\n",
        "        try:\n",
        "            # Extract class and filename\n",
        "            relative_path = os.path.relpath(img_path, DATASET_PATH)\n",
        "            parts = relative_path.split(os.sep)\n",
        "\n",
        "            if len(parts) > 1:\n",
        "                class_name = parts[0]\n",
        "                filename = parts[1]\n",
        "            else:\n",
        "                class_name = \"unknown\"\n",
        "                filename = parts[0]\n",
        "\n",
        "            img_id = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Create output directory for this class\n",
        "            class_output_dir = os.path.join(OUTPUT_DIR, class_name)\n",
        "            os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            orig_width, orig_height = image.size\n",
        "            image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "            if on_gpu:\n",
        "                image_tensor = image_tensor.cuda()\n",
        "\n",
        "            # Run EfficientAD inference\n",
        "            teacher_output = teacher(image_tensor)\n",
        "            teacher_output = (teacher_output - channel_mean) / channel_std\n",
        "            student_output = student(image_tensor)\n",
        "            autoencoder_output = autoencoder(image_tensor)\n",
        "\n",
        "            map_st = torch.mean((teacher_output - student_output[:, :out_channels])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "            map_ae = torch.mean((autoencoder_output - student_output[:, out_channels:])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "\n",
        "            # Apply quantile normalization\n",
        "            map_st = 0.1 * (map_st - q_st_start) / (q_st_end - q_st_start)\n",
        "            map_ae = 0.1 * (map_ae - q_ae_start) / (q_ae_end - q_ae_start)\n",
        "\n",
        "            # Combine maps\n",
        "            map_combined = 0.5 * map_st + 0.5 * map_ae\n",
        "\n",
        "            # Pad and resize back to original dimensions\n",
        "            map_combined = torch.nn.functional.pad(map_combined, (4, 4, 4, 4))\n",
        "            map_combined = torch.nn.functional.interpolate(\n",
        "                map_combined, (orig_height, orig_width), mode='bilinear')\n",
        "\n",
        "            # Convert to numpy\n",
        "            anomaly_map = map_combined[0, 0].cpu().numpy()\n",
        "\n",
        "            # Save as tiff file (for full precision)\n",
        "            tiff_path = os.path.join(class_output_dir, f\"{img_id}.tiff\")\n",
        "            tifffile.imwrite(tiff_path, anomaly_map)\n",
        "\n",
        "            # Also save a visualization\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            # Original image\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(np.array(Image.open(img_path).convert('RGB')))\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Anomaly map\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(anomaly_map, cmap='jet')\n",
        "            plt.title(f\"Anomaly Map (Max: {np.max(anomaly_map):.3f})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Save visualization\n",
        "            vis_path = os.path.join(class_output_dir, f\"{img_id}_vis.png\")\n",
        "            plt.savefig(vis_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing test image {img_path}: {e}\")\n",
        "\n",
        "print(f\"All anomaly maps saved to {OUTPUT_DIR}\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8dbC-m2gpBG",
        "outputId": "b9f75732-72c4-4ace-fb65-4273bad3cb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Path: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Model Path: /content/drive/MyDrive/EfficientAD_/result-2/trainings/mvtec_ad/wood\n",
            "Output Directory: /content/drive/MyDrive/EfficientAD_/result-ground_truth/png\n",
            "Train Directory: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Dataset path exists: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Model path exists: /content/drive/MyDrive/EfficientAD_/result-2/trainings/mvtec_ad/wood\n",
            "Train path exists: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Loading models and quantiles...\n",
            "Successfully loaded final models\n",
            "Loading quantiles...\n",
            "Successfully loaded quantiles\n",
            "Loading training images for normalization...\n",
            "Found 20 images with pattern: /content/drive/MyDrive/Wood_Dataset/wood/train/**/*.jpg\n",
            "Found 20 images with pattern: /content/drive/MyDrive/Wood_Dataset/wood/train/good/*.jpg\n",
            "Total training images found: 40\n",
            "First few training images:\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/8.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/11.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/4.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/9.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/5.jpg\n",
            "Computing teacher normalization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 24.79it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 24.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for test images...\n",
            "Found 71 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/ground_truth/**/*.jpg\n",
            "Found 71 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/*.jpg\n",
            "Found 71 total test images.\n",
            "First few test images:\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900007_mask.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100034_mask.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100026_mask.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900017_mask.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900010_mask.jpg\n",
            "Starting inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:21<00:00,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All anomaly maps saved to /content/drive/MyDrive/EfficientAD_/result-ground_truth/png\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import tifffile\n",
        "import cv2\n",
        "\n",
        "# Fix for PyTorch 2.6+ pickle loading issue\n",
        "from torch.serialization import add_safe_globals\n",
        "add_safe_globals([\n",
        "    'torch.nn.modules.container.Sequential',\n",
        "    'torch.nn.modules.conv.Conv2d',\n",
        "    'torch.nn.modules.activation.ReLU',\n",
        "    'torch.nn.modules.pooling.AvgPool2d',\n",
        "    'torch.nn.modules.linear.Linear',\n",
        "    'torch.nn.modules.normalization.GroupNorm',\n",
        "    'torch.nn.modules.activation.GELU',\n",
        "    'torchvision.models.resnet.Bottleneck',\n",
        "    'collections.OrderedDict'\n",
        "])\n",
        "\n",
        "# Paths - UPDATE THESE TO MATCH YOUR ACTUAL DIRECTORIES\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/wood_otsu/wood/test\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/EfficientAD_/result-13MayÄ±s_/png\"\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"  # Path to training images\n",
        "GT_DIR = \"/content/drive/MyDrive/wood_otsu/wood/ground_truth\"  # Ground truth directory\n",
        "\n",
        "# Contour threshold for anomaly detection - Updated to 0.250\n",
        "CONTOUR_THRESHOLD = 0.250\n",
        "\n",
        "# Print paths for verification\n",
        "print(f\"Dataset Path: {DATASET_PATH}\")\n",
        "print(f\"Model Path: {MODEL_PATH}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"Train Directory: {TRAIN_DIR}\")\n",
        "print(f\"Ground Truth Directory: {GT_DIR}\")\n",
        "print(f\"Contour Threshold: {CONTOUR_THRESHOLD}\")\n",
        "\n",
        "# Check if directories exist\n",
        "for path, name in [(DATASET_PATH, \"Dataset\"), (MODEL_PATH, \"Model\"), (TRAIN_DIR, \"Train\"), (GT_DIR, \"Ground Truth\")]:\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"WARNING: {name} path does not exist: {path}\")\n",
        "    else:\n",
        "        print(f\"{name} path exists: {path}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"grid_views\"), exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "on_gpu = torch.cuda.is_available()\n",
        "out_channels = 384\n",
        "image_size = 256\n",
        "\n",
        "# Default transform for testing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Helper functions for contour detection and border removal\n",
        "def filter_contours(contours, image_shape, min_area=100, border_margin=30):\n",
        "    \"\"\"Filter contours based on size and position\"\"\"\n",
        "    h, w = image_shape[:2]\n",
        "    filtered = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        x, y, cw, ch = cv2.boundingRect(cnt)\n",
        "        if area > min_area and border_margin < x < w-border_margin and border_margin < y < h-border_margin:\n",
        "            filtered.append(cnt)\n",
        "    return filtered\n",
        "\n",
        "def remove_border_artifacts(anomaly_map, border_px=30):\n",
        "    \"\"\"Remove border artifacts from anomaly map\"\"\"\n",
        "    h, w = anomaly_map.shape\n",
        "    mask = np.zeros_like(anomaly_map)\n",
        "    mask[border_px:h-border_px, border_px:w-border_px] = 1\n",
        "    return anomaly_map * mask\n",
        "\n",
        "print(\"Loading models and quantiles...\")\n",
        "try:\n",
        "    # Try loading with weights_only=False to handle PyTorch 2.6 changes\n",
        "    teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_final.pth'), weights_only=False)\n",
        "    student = torch.load(os.path.join(MODEL_PATH, 'student_final.pth'), weights_only=False)\n",
        "    autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_final.pth'), weights_only=False)\n",
        "    print(\"Successfully loaded final models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading final models: {e}\")\n",
        "    try:\n",
        "        # Try temporary models\n",
        "        print(\"Trying temporary models...\")\n",
        "        teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_tmp.pth'), weights_only=False)\n",
        "        student = torch.load(os.path.join(MODEL_PATH, 'student_tmp.pth'), weights_only=False)\n",
        "        autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_tmp.pth'), weights_only=False)\n",
        "        print(\"Successfully loaded temporary models\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporary models: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load quantiles\n",
        "print(\"Loading quantiles...\")\n",
        "try:\n",
        "    quantiles = torch.load(os.path.join(MODEL_PATH, 'map_quantiles_final.pth'), weights_only=False)\n",
        "    q_st_start = torch.tensor(quantiles['q_st_start']) # Convert to tensor\n",
        "    q_st_end = torch.tensor(quantiles['q_st_end'])  # Convert to tensor\n",
        "    q_ae_start = torch.tensor(quantiles['q_ae_start'])  # Convert to tensor\n",
        "    q_ae_end = torch.tensor(quantiles['q_ae_end'])  # Convert to tensor\n",
        "    print(\"Successfully loaded quantiles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantiles: {e}\")\n",
        "    raise\n",
        "\n",
        "if on_gpu:\n",
        "    teacher = teacher.cuda()\n",
        "    student = student.cuda()\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    q_st_start = q_st_start.cuda()\n",
        "    q_st_end = q_st_end.cuda()\n",
        "    q_ae_start = q_ae_start.cuda()\n",
        "    q_ae_end = q_ae_end.cuda()\n",
        "\n",
        "# Set models to evaluation mode\n",
        "teacher.eval()\n",
        "student.eval()\n",
        "autoencoder.eval()\n",
        "\n",
        "# Load train data for computing teacher normalization\n",
        "print(\"Loading training images for normalization...\")\n",
        "\n",
        "# Try different patterns to locate training images\n",
        "train_patterns = [\n",
        "    os.path.join(TRAIN_DIR, \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.png\")\n",
        "]\n",
        "\n",
        "train_images = []\n",
        "for pattern in train_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} images with pattern: {pattern}\")\n",
        "        train_images.extend(found_images)\n",
        "\n",
        "if not train_images:\n",
        "    print(\"ERROR: No training images found! Checking parent directories...\")\n",
        "    # Try to look in parent directories\n",
        "    parent_dir = os.path.dirname(TRAIN_DIR)\n",
        "    for pattern in [os.path.join(parent_dir, \"**\", \"*.jpg\"), os.path.join(parent_dir, \"**\", \"*.png\")]:\n",
        "        found_images = glob(pattern, recursive=True)\n",
        "        if found_images:\n",
        "            print(f\"Found {len(found_images)} images in parent directory with pattern: {pattern}\")\n",
        "            train_images.extend(found_images)\n",
        "\n",
        "# Print first few train images for verification\n",
        "if train_images:\n",
        "    print(f\"Total training images found: {len(train_images)}\")\n",
        "    print(\"First few training images:\")\n",
        "    for i in range(min(5, len(train_images))):\n",
        "        print(f\"  - {train_images[i]}\")\n",
        "else:\n",
        "    print(\"ERROR: Could not find any training images! Cannot proceed without training images.\")\n",
        "    # Instead of crashing, we'll use pre-computed statistics from the quantiles\n",
        "    print(\"Using pre-computed statistics from quantiles to continue...\")\n",
        "    # Create some dummy statistics that should work reasonably well\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "    if on_gpu:\n",
        "        channel_mean = channel_mean.cuda()\n",
        "        channel_std = channel_std.cuda()\n",
        "\n",
        "# If we have training images, compute normalization statistics\n",
        "if train_images:\n",
        "    # If there are too many images, use just a subset for normalization\n",
        "    if len(train_images) > 50:\n",
        "        import random\n",
        "        train_images = random.sample(train_images, 50)\n",
        "        print(f\"Using random subset of 50 images for normalization\")\n",
        "\n",
        "    # Compute teacher normalization\n",
        "    print(\"Computing teacher normalization...\")\n",
        "    mean_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for img_path in tqdm(train_images):\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                image = transform(image).unsqueeze(0)\n",
        "                if on_gpu:\n",
        "                    image = image.cuda()\n",
        "                teacher_output = teacher(image)\n",
        "                mean_output = torch.mean(teacher_output, dim=[0, 2, 3])\n",
        "                mean_outputs.append(mean_output)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing training image {img_path}: {e}\")\n",
        "\n",
        "    if not mean_outputs:\n",
        "        print(\"ERROR: Failed to process any training images!\")\n",
        "        # Use dummy statistics\n",
        "        channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "        channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "        if on_gpu:\n",
        "            channel_mean = channel_mean.cuda()\n",
        "            channel_std = channel_std.cuda()\n",
        "    else:\n",
        "        channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)\n",
        "        channel_mean = channel_mean[None, :, None, None]\n",
        "\n",
        "        mean_distances = []\n",
        "        with torch.no_grad():\n",
        "            for img_path in tqdm(train_images):\n",
        "                try:\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image = transform(image).unsqueeze(0)\n",
        "                    if on_gpu:\n",
        "                        image = image.cuda()\n",
        "                    teacher_output = teacher(image)\n",
        "                    distance = (teacher_output - channel_mean) ** 2\n",
        "                    mean_distance = torch.mean(distance, dim=[0, 2, 3])\n",
        "                    mean_distances.append(mean_distance)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing training image distance {img_path}: {e}\")\n",
        "\n",
        "        channel_var = torch.mean(torch.stack(mean_distances), dim=0)\n",
        "        channel_var = channel_var[None, :, None, None]\n",
        "        channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "# Find all test images (both good and defect)\n",
        "print(\"Looking for test images...\")\n",
        "test_patterns = [\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.png\")\n",
        "]\n",
        "\n",
        "test_images = []\n",
        "for pattern in test_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} test images with pattern: {pattern}\")\n",
        "        test_images.extend(found_images)\n",
        "\n",
        "# Remove duplicates\n",
        "test_images = list(set(test_images))\n",
        "\n",
        "if not test_images:\n",
        "    print(\"ERROR: No test images found!\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Found {len(test_images)} total test images.\")\n",
        "print(\"First few test images:\")\n",
        "for i in range(min(5, len(test_images))):\n",
        "    print(f\"  - {test_images[i]}\")\n",
        "\n",
        "# Run inference on all test images\n",
        "print(\"Starting inference...\")\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(test_images, desc=\"Processing test images\"):\n",
        "        try:\n",
        "            # Extract class and filename\n",
        "            relative_path = os.path.relpath(img_path, DATASET_PATH)\n",
        "            parts = relative_path.split(os.sep)\n",
        "\n",
        "            if len(parts) > 1:\n",
        "                class_name = parts[0]\n",
        "                filename = parts[1]\n",
        "            else:\n",
        "                class_name = \"unknown\"\n",
        "                filename = parts[0]\n",
        "\n",
        "            img_id = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Create output directory for this class\n",
        "            class_output_dir = os.path.join(OUTPUT_DIR, class_name)\n",
        "            os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            orig_width, orig_height = image.size\n",
        "            image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "            if on_gpu:\n",
        "                image_tensor = image_tensor.cuda()\n",
        "\n",
        "            # Run EfficientAD inference\n",
        "            teacher_output = teacher(image_tensor)\n",
        "            teacher_output = (teacher_output - channel_mean) / channel_std\n",
        "            student_output = student(image_tensor)\n",
        "            autoencoder_output = autoencoder(image_tensor)\n",
        "\n",
        "            map_st = torch.mean((teacher_output - student_output[:, :out_channels])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "            map_ae = torch.mean((autoencoder_output - student_output[:, out_channels:])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "\n",
        "            # Apply quantile normalization\n",
        "            map_st = 0.1 * (map_st - q_st_start) / (q_st_end - q_st_start)\n",
        "            map_ae = 0.1 * (map_ae - q_ae_start) / (q_ae_end - q_ae_start)\n",
        "\n",
        "            # Combine maps\n",
        "            map_combined = 0.5 * map_st + 0.5 * map_ae\n",
        "\n",
        "            # Pad and resize back to original dimensions\n",
        "            map_combined = torch.nn.functional.pad(map_combined, (4, 4, 4, 4))\n",
        "            map_combined = torch.nn.functional.interpolate(\n",
        "                map_combined, (orig_height, orig_width), mode='bilinear')\n",
        "\n",
        "            # Convert to numpy\n",
        "            anomaly_map = map_combined[0, 0].cpu().numpy()\n",
        "\n",
        "            # Save as tiff file (for full precision)\n",
        "            tiff_path = os.path.join(class_output_dir, f\"{img_id}.tiff\")\n",
        "            tifffile.imwrite(tiff_path, anomaly_map)\n",
        "\n",
        "            # Load original image for overlay\n",
        "            orig_img = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "            # Remove border artifacts for scoring\n",
        "            anomaly_map_masked = remove_border_artifacts(anomaly_map.copy(), border_px=40)\n",
        "            max_score = np.max(anomaly_map_masked)\n",
        "\n",
        "            # Create overlay with contours\n",
        "            overlay_img = orig_img.copy()\n",
        "\n",
        "            # Create binary mask for contours\n",
        "            binary_mask = (anomaly_map_masked > CONTOUR_THRESHOLD).astype(np.uint8) * 255\n",
        "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            contours = filter_contours(contours, orig_img.shape, min_area=200, border_margin=40)\n",
        "\n",
        "            # Draw contours on overlay\n",
        "            overlay_img_cv = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(overlay_img_cv, contours, -1, (0, 0, 255), 2)  # Red contours\n",
        "\n",
        "            # Add prediction text\n",
        "            prediction = \"Anomali\" if max_score > CONTOUR_THRESHOLD else \"Normal\"\n",
        "            cv2.putText(overlay_img_cv, f\"Tahmin: {prediction} ({max_score:.3f})\",\n",
        "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "            # Convert back to RGB for matplotlib\n",
        "            overlay_img = cv2.cvtColor(overlay_img_cv, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create heatmap visualization\n",
        "            tiff_norm = cv2.normalize(anomaly_map, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "            tiff_colored = cv2.applyColorMap(tiff_norm, cv2.COLORMAP_JET)\n",
        "            tiff_colored = cv2.cvtColor(tiff_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Try to load ground truth mask if available\n",
        "            gt_mask = None\n",
        "            potential_gt_paths = [\n",
        "                os.path.join(GT_DIR, class_name, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, class_name, f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, \"ground_truth\", class_name, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, \"ground_truth\", class_name, f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, f\"{img_id}_mask.jpg\")\n",
        "            ]\n",
        "\n",
        "            for gt_path in potential_gt_paths:\n",
        "                if os.path.exists(gt_path):\n",
        "                    gt_mask = np.array(Image.open(gt_path).convert('RGB'))\n",
        "                    print(f\"Found ground truth mask at: {gt_path}\")\n",
        "                    break\n",
        "\n",
        "            # If ground truth mask not found, create a blank one\n",
        "            if gt_mask is None:\n",
        "                gt_mask = np.ones_like(orig_img) * 255\n",
        "                # Only center part is black\n",
        "                cv2.rectangle(gt_mask, (0, 0), (orig_width, orig_height), (0, 0, 0), -1)\n",
        "                gt_mask[10:-10, 10:-10] = [255, 255, 255]\n",
        "\n",
        "            # Create grid visualization (Original, Anomaly Map, Overlay, Ground Truth)\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Original Image\n",
        "            plt.subplot(1, 4, 1)\n",
        "            plt.imshow(orig_img)\n",
        "            plt.title(f\"{filename} ({class_name}) - Original\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Anomaly Map\n",
        "            plt.subplot(1, 4, 2)\n",
        "            plt.imshow(tiff_colored)\n",
        "            plt.title(f\"Anomaly Map (.tiff) - Max: {max_score:.3f}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Overlay\n",
        "            plt.subplot(1, 4, 3)\n",
        "            plt.imshow(overlay_img)\n",
        "            plt.title(f\"Overlay (Tahmin: {prediction})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Ground Truth\n",
        "            plt.subplot(1, 4, 4)\n",
        "            plt.imshow(gt_mask)\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Save grid visualization\n",
        "            grid_path = os.path.join(OUTPUT_DIR, \"grid_views\", f\"{class_name}_{img_id}_grid.png\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(grid_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # Store result for summary\n",
        "            results.append({\n",
        "                'path': img_path,\n",
        "                'class': class_name,\n",
        "                'filename': filename,\n",
        "                'img_id': img_id,\n",
        "                'score': max_score,\n",
        "                'prediction': prediction,\n",
        "                'has_contours': len(contours) > 0,\n",
        "                'grid_path': grid_path\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing test image {img_path}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Create summary report\n",
        "try:\n",
        "    print(\"Creating summary report...\")\n",
        "    report_path = os.path.join(OUTPUT_DIR, \"summary_report.html\")\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"<html><head>\")\n",
        "        f.write(\"<style>\")\n",
        "        f.write(\"body { font-family: Arial, sans-serif; margin: 20px; }\")\n",
        "        f.write(\"h1 { color: #333; }\")\n",
        "        f.write(\"table { border-collapse: collapse; width: 100%; }\")\n",
        "        f.write(\"th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\")\n",
        "        f.write(\"th { background-color: #f2f2f2; }\")\n",
        "        f.write(\"tr:nth-child(even) { background-color: #f9f9f9; }\")\n",
        "        f.write(\"img { max-width: 1000px; border: 1px solid #ddd; }\")\n",
        "        f.write(\"</style>\")\n",
        "        f.write(\"</head><body>\")\n",
        "        f.write(\"<h1>Anomaly Detection Results</h1>\")\n",
        "\n",
        "        # Summary statistics\n",
        "        good_samples = [r for r in results if r['class'] == 'good']\n",
        "        defect_samples = [r for r in results if r['class'] != 'good']\n",
        "\n",
        "        f.write(f\"<p>Total samples: {len(results)}</p>\")\n",
        "        f.write(f\"<p>Good samples: {len(good_samples)}</p>\")\n",
        "        f.write(f\"<p>Defect samples: {len(defect_samples)}</p>\")\n",
        "\n",
        "        # Table of results\n",
        "        f.write(\"<h2>Results Table</h2>\")\n",
        "        f.write(\"<table>\")\n",
        "        f.write(\"<tr><th>ID</th><th>Class</th><th>Score</th><th>Prediction</th></tr>\")\n",
        "\n",
        "        for r in sorted(results, key=lambda x: x['score'], reverse=True):\n",
        "            f.write(f\"<tr>\")\n",
        "            f.write(f\"<td>{r['img_id']}</td>\")\n",
        "            f.write(f\"<td>{r['class']}</td>\")\n",
        "            f.write(f\"<td>{r['score']:.4f}</td>\")\n",
        "            f.write(f\"<td>{r['prediction']}</td>\")\n",
        "            f.write(f\"</tr>\")\n",
        "\n",
        "        f.write(\"</table>\")\n",
        "\n",
        "        # Images\n",
        "        f.write(\"<h2>Top Anomalies</h2>\")\n",
        "\n",
        "        # Sort by score and take top 20\n",
        "        top_anomalies = sorted(results, key=lambda x: x['score'], reverse=True)[:20]\n",
        "\n",
        "        for r in top_anomalies:\n",
        "            f.write(f\"<h3>{r['filename']} (Class: {r['class']}, Score: {r['score']:.4f})</h3>\")\n",
        "            f.write(f\"<img src='{os.path.relpath(r['grid_path'], OUTPUT_DIR)}'>\")\n",
        "            f.write(\"<hr>\")\n",
        "\n",
        "        f.write(\"</body></html>\")\n",
        "\n",
        "    print(f\"Summary report created at: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating summary report: {e}\")\n",
        "\n",
        "print(f\"All anomaly maps and visualizations saved to {OUTPUT_DIR}\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69HHaNRQwtHA",
        "outputId": "02bee49e-6f49-4068-80db-a422d962731e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Path: /content/drive/MyDrive/wood_otsu/wood/test\n",
            "Model Path: /content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\n",
            "Output Directory: /content/drive/MyDrive/EfficientAD_/result-final_/png\n",
            "Train Directory: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Ground Truth Directory: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Contour Threshold: 0.25\n",
            "Dataset path exists: /content/drive/MyDrive/wood_otsu/wood/test\n",
            "Model path exists: /content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\n",
            "Train path exists: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Ground Truth path exists: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Loading models and quantiles...\n",
            "Successfully loaded final models\n",
            "Loading quantiles...\n",
            "Successfully loaded quantiles\n",
            "Loading training images for normalization...\n",
            "Found 20 images with pattern: /content/drive/MyDrive/Wood_Dataset/wood/train/**/*.jpg\n",
            "Found 20 images with pattern: /content/drive/MyDrive/Wood_Dataset/wood/train/good/*.jpg\n",
            "Total training images found: 40\n",
            "First few training images:\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/8.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/11.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/4.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/9.jpg\n",
            "  - /content/drive/MyDrive/Wood_Dataset/wood/train/good/5.jpg\n",
            "Computing teacher normalization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 24.92it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 24.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for test images...\n",
            "Found 140 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/**/*.jpg\n",
            "Found 70 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/good/*.jpg\n",
            "Found 70 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/defect/*.jpg\n",
            "Found 140 total test images.\n",
            "First few test images:\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/113.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/defect/101100012.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/defect/100000000.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/defect/101500001.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/102.jpg\n",
            "Starting inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:   1%|          | 1/140 [00:00<01:39,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100012_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:   1%|â–         | 2/140 [00:01<01:52,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000000_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:   2%|â–         | 3/140 [00:02<01:56,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101500001_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:   7%|â–‹         | 10/140 [00:07<01:34,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000006_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:   9%|â–Š         | 12/140 [00:09<01:35,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100059_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:   9%|â–‰         | 13/140 [00:10<01:38,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100500053_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  11%|â–ˆ         | 15/140 [00:11<01:38,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103200067_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  12%|â–ˆâ–        | 17/140 [00:13<01:35,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900003_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  14%|â–ˆâ–        | 20/140 [00:15<01:29,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000008_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  15%|â–ˆâ–Œ        | 21/140 [00:16<01:32,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100038_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  19%|â–ˆâ–Š        | 26/140 [00:19<01:19,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000021_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  19%|â–ˆâ–‰        | 27/140 [00:20<01:25,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000016_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  24%|â–ˆâ–ˆâ–Ž       | 33/140 [00:25<01:14,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101800003_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  24%|â–ˆâ–ˆâ–       | 34/140 [00:26<01:19,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900001_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  26%|â–ˆâ–ˆâ–Œ       | 36/140 [00:27<01:19,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100037_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  27%|â–ˆâ–ˆâ–‹       | 38/140 [00:29<01:16,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000007_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  29%|â–ˆâ–ˆâ–‰       | 41/140 [00:31<01:13,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101800000_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  31%|â–ˆâ–ˆâ–ˆ       | 43/140 [00:32<01:13,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103200049_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  32%|â–ˆâ–ˆâ–ˆâ–      | 45/140 [00:35<01:40,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100026_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 46/140 [00:36<01:33,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000012_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 47/140 [00:37<01:28,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101500002_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  34%|â–ˆâ–ˆâ–ˆâ–      | 48/140 [00:38<01:25,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100003_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 50/140 [00:40<01:16,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101700027_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 52/140 [00:41<01:11,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103500050_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 55/140 [00:43<01:04,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103200050_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 56/140 [00:44<01:06,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101500054_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 57/140 [00:45<01:07,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900013_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 59/140 [00:47<01:06,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000073_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 63/140 [00:50<00:58,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100035_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 64/140 [00:51<00:59,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100052_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 65/140 [00:52<01:00,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100034_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 66/140 [00:52<01:01,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000018_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 67/140 [00:53<01:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100043_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 69/140 [00:55<01:04,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100002_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 72/140 [00:58<00:55,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000056_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 73/140 [00:59<00:55,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100036_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 74/140 [01:00<00:56,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103400075_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 75/140 [01:00<00:57,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000009_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 76/140 [01:01<00:56,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100048_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 79/140 [01:04<00:47,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103400051_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 81/140 [01:05<00:44,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000020_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 82/140 [01:06<00:48,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100700054_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 84/140 [01:08<00:44,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100027_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 86/140 [01:09<00:41,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100005_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 88/140 [01:11<00:40,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000071_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 89/140 [01:12<00:41,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103400076_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 91/140 [01:13<00:38,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100033_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 92/140 [01:14<00:38,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100062_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 94/140 [01:16<00:36,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900007_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 97/140 [01:19<00:38,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900012_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 98/140 [01:19<00:37,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100071_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 102/140 [01:22<00:28,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100083_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 107/140 [01:26<00:23,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900004_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 109/140 [01:28<00:22,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900005_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 110/140 [01:28<00:23,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000014_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 116/140 [01:33<00:16,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900010_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 117/140 [01:34<00:17,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000003_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 120/140 [01:36<00:15,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/103200070_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 121/140 [01:37<00:15,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900009_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 124/140 [01:39<00:12,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900002_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 125/140 [01:40<00:11,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100039_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 126/140 [01:41<00:11,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100400000_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 130/140 [01:44<00:07,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100004_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 131/140 [01:45<00:06,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101100013_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 133/140 [01:46<00:05,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100008_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 134/140 [01:48<00:05,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100700037_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 136/140 [01:49<00:03,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900016_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 137/140 [01:50<00:02,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000069_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 138/140 [01:51<00:01,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100500011_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 139/140 [01:52<00:00,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900017_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [01:53<00:00,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating summary report...\n",
            "Summary report created at: /content/drive/MyDrive/EfficientAD_/result-final_/png/summary_report.html\n",
            "All anomaly maps and visualizations saved to /content/drive/MyDrive/EfficientAD_/result-final_/png\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: modeli kaydeder misin arayÃ¼z yaparken kullanmam gerekcek\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np # Needed for dummy variables if training images are not found\n",
        "from PIL import Image # Needed for loading dummy images if needed\n",
        "from torchvision import transforms # Needed for loading dummy transform if needed\n",
        "from glob import glob # Needed for finding training images\n",
        "from tqdm import tqdm # Needed for progress bar during normalization computation\n",
        "\n",
        "# Fix for PyTorch 2.6+ pickle loading issue - Ensure this is also in the saving cell\n",
        "from torch.serialization import add_safe_globals\n",
        "add_safe_globals([\n",
        "    'torch.nn.modules.container.Sequential',\n",
        "    'torch.nn.modules.conv.Conv2d',\n",
        "    'torch.nn.modules.activation.ReLU',\n",
        "    'torch.nn.modules.pooling.AvgPool2d',\n",
        "    'torch.nn.modules.linear.Linear',\n",
        "    'torch.nn.modules.normalization.GroupNorm',\n",
        "    'torch.nn.modules.activation.GELU',\n",
        "    'torchvision.models.resnet.Bottleneck',\n",
        "    'collections.OrderedDict'\n",
        "])\n",
        "\n",
        "\n",
        "# Define paths again to make sure they are available in this cell\n",
        "MODEL_PATH = \"/content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\" # Replace with the correct model path\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"  # Path to training images\n",
        "\n",
        "# Constants (ensure these are also defined)\n",
        "on_gpu = torch.cuda.is_available()\n",
        "out_channels = 384\n",
        "image_size = 256\n",
        "\n",
        "# Default transform (ensure this is also defined)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Loading models and quantiles for saving...\")\n",
        "try:\n",
        "    # Try loading with weights_only=False to handle PyTorch 2.6 changes\n",
        "    teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_final.pth'), weights_only=False)\n",
        "    student = torch.load(os.path.join(MODEL_PATH, 'student_final.pth'), weights_only=False)\n",
        "    autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_final.pth'), weights_only=False)\n",
        "    print(\"Successfully loaded final models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading final models: {e}\")\n",
        "    try:\n",
        "        # Try temporary models\n",
        "        print(\"Trying temporary models...\")\n",
        "        teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_tmp.pth'), weights_only=False)\n",
        "        student = torch.load(os.path.join(MODEL_PATH, 'student_tmp.pth'), weights_only=False)\n",
        "        autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_tmp.pth'), weights_only=False)\n",
        "        print(\"Successfully loaded temporary models\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporary models: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Load quantiles\n",
        "print(\"Loading quantiles for saving...\")\n",
        "try:\n",
        "    quantiles = torch.load(os.path.join(MODEL_PATH, 'map_quantiles_final.pth'), weights_only=False)\n",
        "    q_st_start = torch.tensor(quantiles['q_st_start']) # Convert to tensor\n",
        "    q_st_end = torch.tensor(quantiles['q_st_end'])  # Convert to tensor\n",
        "    q_ae_start = torch.tensor(quantiles['q_ae_start'])  # Convert to tensor\n",
        "    q_ae_end = torch.tensor(quantiles['q_ae_end'])  # Convert to tensor\n",
        "    print(\"Successfully loaded quantiles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantiles: {e}\")\n",
        "    raise\n",
        "\n",
        "if on_gpu:\n",
        "    teacher = teacher.cuda()\n",
        "    student = student.cuda()\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    q_st_start = q_st_start.cuda()\n",
        "    q_st_end = q_st_end.cuda()\n",
        "    q_ae_start = q_ae_start.cuda()\n",
        "    q_ae_end = q_ae_end.cuda()\n",
        "\n",
        "# Set models to evaluation mode (important for consistent behavior)\n",
        "teacher.eval()\n",
        "student.eval()\n",
        "autoencoder.eval()\n",
        "\n",
        "# Load train data for computing teacher normalization if not already computed\n",
        "# This part is crucial to ensure channel_mean and channel_std are defined\n",
        "print(\"Loading training images for normalization (if needed)...\")\n",
        "\n",
        "train_patterns = [\n",
        "    os.path.join(TRAIN_DIR, \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.png\")\n",
        "]\n",
        "\n",
        "train_images = []\n",
        "for pattern in train_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        train_images.extend(found_images)\n",
        "\n",
        "if not train_images:\n",
        "    print(\"WARNING: No training images found. Using dummy statistics.\")\n",
        "    # Create some dummy statistics that should work reasonably well\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "    if on_gpu:\n",
        "        channel_mean = channel_mean.cuda()\n",
        "        channel_std = channel_std.cuda()\n",
        "else:\n",
        "    # If we have training images, compute normalization statistics\n",
        "    print(\"Computing teacher normalization...\")\n",
        "    mean_outputs = []\n",
        "    with torch.no_grad():\n",
        "        # Limit training images for speed if too many\n",
        "        sample_images = train_images[:min(50, len(train_images))] # Use a subset\n",
        "        for img_path in tqdm(sample_images, desc=\"Computing mean\"):\n",
        "             try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                image = transform(image).unsqueeze(0)\n",
        "                if on_gpu:\n",
        "                    image = image.cuda()\n",
        "                teacher_output = teacher(image)\n",
        "                mean_output = torch.mean(teacher_output, dim=[0, 2, 3])\n",
        "                mean_outputs.append(mean_output)\n",
        "             except Exception as e:\n",
        "                print(f\"Error processing training image {img_path} for mean: {e}\")\n",
        "\n",
        "\n",
        "    if not mean_outputs:\n",
        "         print(\"ERROR: Failed to process any training images for mean!\")\n",
        "         # Use dummy statistics\n",
        "         channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "         channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "         if on_gpu:\n",
        "             channel_mean = channel_mean.cuda()\n",
        "             channel_std = channel_std.cuda()\n",
        "    else:\n",
        "        channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)\n",
        "        channel_mean = channel_mean[None, :, None, None]\n",
        "\n",
        "        mean_distances = []\n",
        "        with torch.no_grad():\n",
        "             # Limit training images for speed if too many\n",
        "            sample_images = train_images[:min(50, len(train_images))] # Use a subset\n",
        "            for img_path in tqdm(sample_images, desc=\"Computing std\"):\n",
        "                try:\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image = transform(image).unsqueeze(0)\n",
        "                    if on_gpu:\n",
        "                        image = image.cuda()\n",
        "                    teacher_output = teacher(image)\n",
        "                    distance = (teacher_output - channel_mean) ** 2\n",
        "                    mean_distance = torch.mean(distance, dim=[0, 2, 3])\n",
        "                    mean_distances.append(mean_distance)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing training image {img_path} for std: {e}\")\n",
        "\n",
        "        if not mean_distances:\n",
        "            print(\"ERROR: Failed to process any training images for std!\")\n",
        "            # Use dummy statistics (should already be set, but just in case)\n",
        "            channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "            if on_gpu:\n",
        "                channel_std = channel_std.cuda()\n",
        "        else:\n",
        "            channel_var = torch.mean(torch.stack(mean_distances), dim=0)\n",
        "            channel_var = channel_var[None, :, None, None]\n",
        "            channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "\n",
        "# Define the path where you want to save the combined model\n",
        "combined_model_path = os.path.join(MODEL_PATH, 'efficientad_combined_model.pth')\n",
        "\n",
        "# Create a dictionary containing the models and any necessary parameters\n",
        "# Include channel_mean and channel_std if they are needed for inference\n",
        "# Include quantiles if they are needed for inference\n",
        "model_state = {\n",
        "    'teacher': teacher.state_dict(),\n",
        "    'student': student.state_dict(),\n",
        "    'autoencoder': autoencoder.state_dict(),\n",
        "    'channel_mean': channel_mean.cpu(), # Move to CPU before saving if it's on GPU\n",
        "    'channel_std': channel_std.cpu(),   # Move to CPU before saving if it's on GPU\n",
        "    'q_st_start': q_st_start.cpu(),     # Move to CPU before saving if it's on GPU\n",
        "    'q_st_end': q_st_end.cpu(),         # Move to CPU before saving if it's on GPU\n",
        "    'q_ae_start': q_ae_start.cpu(),     # Move to CPU before saving if it's on GPU\n",
        "    'q_ae_end': q_ae_end.cpu(),         # Move to CPU before saving if it's on GPU\n",
        "    'out_channels': out_channels, # Include constants needed for inference\n",
        "    'image_size': image_size,     # Include constants needed for inference\n",
        "    'transform': transform        # Include the transform if it's part of the model logic\n",
        "}\n",
        "\n",
        "# Save the combined state dictionary\n",
        "torch.save(model_state, combined_model_path)\n",
        "\n",
        "print(f\"Model saved successfully to: {combined_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKKyIvisYsXF",
        "outputId": "02c1db98-e4f9-4931-c339-0e13811ccd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models and quantiles for saving...\n",
            "Successfully loaded final models\n",
            "Loading quantiles for saving...\n",
            "Successfully loaded quantiles\n",
            "Loading training images for normalization (if needed)...\n",
            "WARNING: No training images found. Using dummy statistics.\n",
            "Model saved successfully to: /content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood/efficientad_combined_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Modeli yÃ¼kleyip test defect ve good klasÃ¶rÃ¼nden beÅŸer resme sonuÃ§ verdirtmek istiyorum\n",
        "import random\n",
        "import os # Ensure os is imported\n",
        "import torch # Ensure torch is imported\n",
        "import numpy as np # Ensure numpy is imported\n",
        "import matplotlib.pyplot as plt # Ensure matplotlib is imported\n",
        "from PIL import Image # Ensure PIL is imported\n",
        "from glob import glob # Ensure glob is imported\n",
        "from tqdm import tqdm # Ensure tqdm is imported\n",
        "from torchvision import transforms # Ensure transforms is imported\n",
        "import tifffile # Ensure tifffile is imported\n",
        "import cv2 # Ensure cv2 is imported\n",
        "import traceback # Import the traceback module\n",
        "\n",
        "\n",
        "# Fix for PyTorch 2.6+ pickle loading issue - Ensure this is also in the saving cell\n",
        "from torch.serialization import add_safe_globals\n",
        "add_safe_globals([\n",
        "    'torch.nn.modules.container.Sequential',\n",
        "    'torch.nn.modules.conv.Conv2d',\n",
        "    'torch.nn.modules.activation.ReLU',\n",
        "    'torch.nn.modules.pooling.AvgPool2d',\n",
        "    'torch.nn.modules.linear.Linear',\n",
        "    'torch.nn.modules.normalization.GroupNorm',\n",
        "    'torch.nn.modules.activation.GELU',\n",
        "    'torchvision.models.resnet.Bottleneck',\n",
        "    'collections.OrderedDict'\n",
        "])\n",
        "\n",
        "\n",
        "# Define paths again to make sure they are available in this cell\n",
        "# Redefine DATASET_PATH\n",
        "DATASET_PATH = \"/content/drive/MyDrive/wood_otsu/wood/test\" # Define DATASET_PATH here\n",
        "MODEL_PATH = \"/content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\" # Replace with the correct model path\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"  # Path to training images\n",
        "GT_DIR = \"/content/drive/MyDrive/wood_otsu/wood/ground_truth\"  # Ground truth directory\n",
        "\n",
        "# Paths for testing specific images\n",
        "GOOD_IMAGES_DIR = os.path.join(DATASET_PATH, \"good\") # Use DATASET_PATH\n",
        "DEFECT_IMAGES_DIR = os.path.join(DATASET_PATH, \"defect\") # Use DATASET_PATH\n",
        "OUTPUT_DIR_SPECIFIC = \"/content/drive/MyDrive/EfficientAD_/result-specific_images/png\" # Separate output for specific tests\n",
        "\n",
        "# Contour threshold for anomaly detection - Updated to 0.250\n",
        "CONTOUR_THRESHOLD = 0.130 # Ensure this is defined\n",
        "\n",
        "\n",
        "# Number of images to test from each category\n",
        "NUM_IMAGES_TO_TEST = 5\n",
        "\n",
        "print(f\"Testing {NUM_IMAGES_TO_TEST} good images from: {GOOD_IMAGES_DIR}\")\n",
        "print(f\"Testing {NUM_IMAGES_TO_TEST} defect images from: {DEFECT_IMAGES_DIR}\")\n",
        "os.makedirs(os.path.join(OUTPUT_DIR_SPECIFIC, \"grid_views\"), exist_ok=True)\n",
        "\n",
        "# Find good images\n",
        "good_image_patterns = [\n",
        "    os.path.join(GOOD_IMAGES_DIR, \"*.jpg\"),\n",
        "    os.path.join(GOOD_IMAGES_DIR, \"*.png\")\n",
        "]\n",
        "all_good_images = []\n",
        "for pattern in good_image_patterns:\n",
        "    all_good_images.extend(glob(pattern))\n",
        "\n",
        "# Find defect images\n",
        "defect_image_patterns = [\n",
        "    os.path.join(DEFECT_IMAGES_DIR, \"*.jpg\"),\n",
        "    os.path.join(DEFECT_IMAGES_DIR, \"*.png\")\n",
        "]\n",
        "all_defect_images = []\n",
        "for pattern in defect_image_patterns:\n",
        "    all_defect_images.extend(glob(pattern))\n",
        "\n",
        "# Select a subset of images\n",
        "selected_good_images = random.sample(all_good_images, min(NUM_IMAGES_TO_TEST, len(all_good_images)))\n",
        "selected_defect_images = random.sample(all_defect_images, min(NUM_IMAGES_TO_TEST, len(all_defect_images)))\n",
        "\n",
        "print(f\"Selected {len(selected_good_images)} good images.\")\n",
        "print(f\"Selected {len(selected_defect_images)} defect images.\")\n",
        "\n",
        "# Combine selected images\n",
        "selected_test_images = selected_good_images + selected_defect_images\n",
        "\n",
        "# Constants (ensure these are also defined or loaded)\n",
        "on_gpu = torch.cuda.is_available() # Ensure on_gpu is defined\n",
        "out_channels = 384 # Ensure out_channels is defined\n",
        "image_size = 256 # Ensure image_size is defined\n",
        "\n",
        "# Default transform (ensure this is also defined or loaded)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load models and quantiles (ensure this block is present and working)\n",
        "print(\"Loading models and quantiles...\")\n",
        "try:\n",
        "    # Try loading with weights_only=False to handle PyTorch 2.6 changes\n",
        "    teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_final.pth'), weights_only=False)\n",
        "    student = torch.load(os.path.join(MODEL_PATH, 'student_final.pth'), weights_only=False)\n",
        "    autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_final.pth'), weights_only=False)\n",
        "    print(\"Successfully loaded final models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading final models: {e}\")\n",
        "    try:\n",
        "        # Try temporary models\n",
        "        print(\"Trying temporary models...\")\n",
        "        teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_tmp.pth'), weights_only=False)\n",
        "        student = torch.load(os.path.join(MODEL_PATH, 'student_tmp.pth'), weights_only=False)\n",
        "        autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_tmp.pth'), weights_only=False)\n",
        "        print(\"Successfully loaded temporary models\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporary models: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Load quantiles\n",
        "print(\"Loading quantiles...\")\n",
        "try:\n",
        "    quantiles = torch.load(os.path.join(MODEL_PATH, 'map_quantiles_final.pth'), weights_only=False)\n",
        "    q_st_start = torch.tensor(quantiles['q_st_start']) # Convert to tensor\n",
        "    q_st_end = torch.tensor(quantiles['q_st_end'])  # Convert to tensor\n",
        "    q_ae_start = torch.tensor(quantiles['q_ae_start'])  # Convert to tensor\n",
        "    q_ae_end = torch.tensor(quantiles['q_ae_end'])  # Convert to tensor\n",
        "    print(\"Successfully loaded quantiles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantiles: {e}\")\n",
        "    raise\n",
        "\n",
        "if on_gpu:\n",
        "    teacher = teacher.cuda()\n",
        "    student = student.cuda()\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    q_st_start = q_st_start.cuda()\n",
        "    q_st_end = q_st_end.cuda()\n",
        "    q_ae_start = q_ae_start.cuda()\n",
        "    q_ae_end = q_ae_end.cuda()\n",
        "\n",
        "# Set models to evaluation mode (important for consistent behavior)\n",
        "teacher.eval()\n",
        "student.eval()\n",
        "autoencoder.eval()\n",
        "\n",
        "\n",
        "# Load train data for computing teacher normalization if not already computed\n",
        "# This part is crucial to ensure channel_mean and channel_std are defined\n",
        "print(\"Loading training images for normalization (if needed)...\")\n",
        "\n",
        "train_patterns = [\n",
        "    os.path.join(TRAIN_DIR, \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.png\")\n",
        "]\n",
        "\n",
        "train_images = []\n",
        "for pattern in train_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        train_images.extend(found_images)\n",
        "\n",
        "if not train_images:\n",
        "    print(\"WARNING: No training images found. Using dummy statistics.\")\n",
        "    # Create some dummy statistics that should work reasonably well\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "    if on_gpu:\n",
        "        channel_mean = channel_mean.cuda()\n",
        "        channel_std = channel_std.cuda()\n",
        "else:\n",
        "    # If we have training images, compute normalization statistics\n",
        "    print(\"Computing teacher normalization...\")\n",
        "    mean_outputs = []\n",
        "    with torch.no_grad():\n",
        "        # Limit training images for speed if too many\n",
        "        sample_images = train_images[:min(50, len(train_images))] # Use a subset\n",
        "        for img_path_norm in tqdm(sample_images, desc=\"Computing mean\"): # Use a different variable name\n",
        "             try:\n",
        "                image = Image.open(img_path_norm).convert('RGB')\n",
        "                image = transform(image).unsqueeze(0)\n",
        "                if on_gpu:\n",
        "                    image = image.cuda()\n",
        "                teacher_output = teacher(image)\n",
        "                mean_output = torch.mean(teacher_output, dim=[0, 2, 3])\n",
        "                mean_outputs.append(mean_output)\n",
        "             except Exception as e:\n",
        "                print(f\"Error processing training image {img_path_norm} for mean: {e}\")\n",
        "\n",
        "\n",
        "    if not mean_outputs:\n",
        "         print(\"ERROR: Failed to process any training images for mean!\")\n",
        "         # Use dummy statistics\n",
        "         channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "         channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "         if on_gpu:\n",
        "             channel_mean = channel_mean.cuda()\n",
        "             channel_std = channel_std.cuda()\n",
        "    else:\n",
        "        channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)\n",
        "        channel_mean = channel_mean[None, :, None, None]\n",
        "\n",
        "        mean_distances = []\n",
        "        with torch.no_grad():\n",
        "             # Limit training images for speed if too many\n",
        "            sample_images = train_images[:min(50, len(train_images))] # Use a subset\n",
        "            for img_path_norm in tqdm(sample_images, desc=\"Computing std\"): # Use a different variable name\n",
        "                try:\n",
        "                    image = Image.open(img_path_norm).convert('RGB')\n",
        "                    image = transform(image).unsqueeze(0)\n",
        "                    if on_gpu:\n",
        "                        image = image.cuda()\n",
        "                    teacher_output = teacher(image)\n",
        "                    distance = (teacher_output - channel_mean) ** 2\n",
        "                    mean_distance = torch.mean(distance, dim=[0, 2, 3])\n",
        "                    mean_distances.append(mean_distance)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing training image {img_path_norm} for std: {e}\")\n",
        "\n",
        "        if not mean_distances:\n",
        "            print(\"ERROR: Failed to process any training images for std!\")\n",
        "            # Use dummy statistics (should already be set, but just in case)\n",
        "            channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "            if on_gpu:\n",
        "                channel_std = channel_std.cuda()\n",
        "        else:\n",
        "            channel_var = torch.mean(torch.stack(mean_distances), dim=0)\n",
        "            channel_var = channel_var[None, :, None, None]\n",
        "            channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "\n",
        "# Helper functions for contour detection and border removal (ensure these are defined or copied)\n",
        "def filter_contours(contours, image_shape, min_area=100, border_margin=30):\n",
        "    \"\"\"Filter contours based on size and position\"\"\"\n",
        "    h, w = image_shape[:2]\n",
        "    filtered = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        x, y, cw, ch = cv2.boundingRect(cnt)\n",
        "        if area > min_area and border_margin < x < w-border_margin and border_margin < y < h-border_margin:\n",
        "            filtered.append(cnt)\n",
        "    return filtered\n",
        "\n",
        "def remove_border_artifacts(anomaly_map, border_px=30):\n",
        "    \"\"\"Remove border artifacts from anomaly map\"\"\"\n",
        "    h, w = anomaly_map.shape\n",
        "    mask = np.zeros_like(anomaly_map)\n",
        "    mask[border_px:h-border_px, border_px:w-border_px] = 1\n",
        "    return anomaly_map * mask\n",
        "\n",
        "\n",
        "if not selected_test_images:\n",
        "    print(\"No specific test images found.\")\n",
        "else:\n",
        "    print(\"Starting inference on selected images...\")\n",
        "    specific_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_path in tqdm(selected_test_images, desc=\"Processing selected test images\"):\n",
        "            try:\n",
        "                # Extract class and filename\n",
        "                # This line was causing the error, ensure DATASET_PATH is defined above\n",
        "                relative_path = os.path.relpath(img_path, DATASET_PATH)\n",
        "                parts = relative_path.split(os.sep)\n",
        "\n",
        "                if len(parts) > 1:\n",
        "                    class_name = parts[-2] # Correctly get class name from the parent folder\n",
        "                    filename = parts[-1]   # Correctly get filename\n",
        "                else:\n",
        "                    class_name = \"unknown\" # Default to unknown if path structure is unexpected\n",
        "                    filename = parts[-1]\n",
        "\n",
        "                img_id = os.path.splitext(filename)[0]\n",
        "\n",
        "                # Create output directory for this class in the specific output location\n",
        "                class_output_dir = os.path.join(OUTPUT_DIR_SPECIFIC, class_name)\n",
        "                os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "                # Load and preprocess image\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                orig_width, orig_height = image.size\n",
        "                image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "                if on_gpu:\n",
        "                    image_tensor = image_tensor.cuda()\n",
        "\n",
        "                # Run EfficientAD inference\n",
        "                teacher_output = teacher(image_tensor)\n",
        "                # Use the globally computed channel_mean and channel_std\n",
        "                teacher_output = (teacher_output - channel_mean) / channel_std\n",
        "                student_output = student(image_tensor)\n",
        "                autoencoder_output = autoencoder(image_tensor)\n",
        "\n",
        "                map_st = torch.mean((teacher_output - student_output[:, :out_channels])**2,\n",
        "                                   dim=1, keepdim=True)\n",
        "                map_ae = torch.mean((autoencoder_output - student_output[:, out_channels:])**2,\n",
        "                                   dim=1, keepdim=True)\n",
        "\n",
        "                # Apply quantile normalization\n",
        "                # Use the globally loaded quantiles\n",
        "                map_st = 0.1 * (map_st - q_st_start) / (q_st_end - q_st_start)\n",
        "                map_ae = 0.1 * (map_ae - q_ae_start) / (q_ae_end - q_ae_start)\n",
        "\n",
        "                # Combine maps\n",
        "                map_combined = 0.5 * map_st + 0.5 * map_ae\n",
        "\n",
        "                # Pad and resize back to original dimensions\n",
        "                map_combined = torch.nn.functional.pad(map_combined, (4, 4, 4, 4))\n",
        "                map_combined = torch.nn.functional.interpolate(\n",
        "                    map_combined, (orig_height, orig_width), mode='bilinear')\n",
        "\n",
        "                # Convert to numpy\n",
        "                anomaly_map = map_combined[0, 0].cpu().numpy()\n",
        "\n",
        "                # Save as tiff file (for full precision)\n",
        "                tiff_path = os.path.join(class_output_dir, f\"{img_id}.tiff\")\n",
        "                tifffile.imwrite(tiff_path, anomaly_map)\n",
        "\n",
        "                # Load original image for overlay\n",
        "                orig_img = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "                # Remove border artifacts for scoring\n",
        "                anomaly_map_masked = remove_border_artifacts(anomaly_map.copy(), border_px=40)\n",
        "                max_score = np.max(anomaly_map_masked)\n",
        "\n",
        "                # Create overlay with contours\n",
        "                overlay_img = orig_img.copy()\n",
        "\n",
        "                # Create binary mask for contours\n",
        "                binary_mask = (anomaly_map_masked > CONTOUR_THRESHOLD).astype(np.uint8) * 255\n",
        "                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                contours = filter_contours(contours, orig_img.shape, min_area=200, border_margin=40)\n",
        "\n",
        "                # Draw contours on overlay\n",
        "                overlay_img_cv = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
        "                cv2.drawContours(overlay_img_cv, contours, -1, (0, 0, 255), 2)  # Red contours\n",
        "\n",
        "                # Add prediction text\n",
        "                prediction = \"Anomali\" if max_score > CONTOUR_THRESHOLD else \"Normal\"\n",
        "                cv2.putText(overlay_img_cv, f\"Tahmin: {prediction} ({max_score:.3f})\",\n",
        "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "                # Convert back to RGB for matplotlib\n",
        "                overlay_img = cv2.cvtColor(overlay_img_cv, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Create heatmap visualization\n",
        "                tiff_norm = cv2.normalize(anomaly_map, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "                tiff_colored = cv2.applyColorMap(tiff_norm, cv2.COLORMAP_JET)\n",
        "                tiff_colored = cv2.cvtColor(tiff_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "                # Try to load ground truth mask if available\n",
        "                gt_mask = None\n",
        "                # Modify potential_gt_paths to specifically look in the defect ground_truth folder if class_name is defect\n",
        "                if class_name == 'defect':\n",
        "                     potential_gt_paths = [\n",
        "                        os.path.join(GT_DIR, class_name, f\"{img_id}_mask.png\"),\n",
        "                        os.path.join(GT_DIR, class_name, f\"{img_id}_mask.jpg\"),\n",
        "                        os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.png\"), # Look directly in 'defect' if it's a subfolder\n",
        "                        os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.jpg\"),\n",
        "                        os.path.join(GT_DIR, f\"{img_id}_mask.png\"), # Also check root of GT_DIR\n",
        "                        os.path.join(GT_DIR, f\"{img_id}_mask.jpg\")\n",
        "                    ]\n",
        "                else: # For 'good' or 'unknown', create a blank mask\n",
        "                    potential_gt_paths = [] # No ground truth mask expected for 'good' images\n",
        "\n",
        "                for gt_path in potential_gt_paths:\n",
        "                    if os.path.exists(gt_path):\n",
        "                        try:\n",
        "                            # Ensure GT mask is grayscale and convert to 3 channel for display\n",
        "                            gt_mask_raw = np.array(Image.open(gt_path).convert('L'))\n",
        "                            gt_mask = cv2.cvtColor(gt_mask_raw, cv2.COLOR_GRAY2RGB)\n",
        "                            print(f\"Found ground truth mask at: {gt_path}\")\n",
        "                            break # Found a valid mask, stop searching\n",
        "                        except Exception as gt_e:\n",
        "                             print(f\"Error loading ground truth mask {gt_path}: {gt_e}\")\n",
        "                             gt_mask = None # Reset if loading fails\n",
        "\n",
        "\n",
        "                # If ground truth mask not found or loading failed, create a blank one\n",
        "                if gt_mask is None:\n",
        "                    print(f\"Ground truth mask not found for {img_path}. Creating blank mask.\")\n",
        "                    gt_mask = np.ones_like(orig_img) * 255\n",
        "                    # Optionally, draw a subtle border or text to indicate it's a blank GT\n",
        "                    cv2.putText(gt_mask, \"No GT Mask\", (10, orig_height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "\n",
        "                # Create grid visualization (Original, Anomaly Map, Overlay, Ground Truth)\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # Original Image\n",
        "                plt.subplot(1, 4, 1)\n",
        "                plt.imshow(orig_img)\n",
        "                plt.title(f\"{filename} ({class_name}) - Original\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Anomaly Map\n",
        "                plt.subplot(1, 4, 2)\n",
        "                plt.imshow(tiff_colored)\n",
        "                plt.title(f\"Anomaly Map (.tiff) - Max: {max_score:.3f}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Overlay\n",
        "                plt.subplot(1, 4, 3)\n",
        "                plt.imshow(overlay_img)\n",
        "                plt.title(f\"Overlay (Tahmin: {prediction})\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Ground Truth\n",
        "                plt.subplot(1, 4, 4)\n",
        "                plt.imshow(gt_mask)\n",
        "                plt.title(\"Ground Truth\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Save grid visualization to the specific output directory\n",
        "                grid_path_specific = os.path.join(OUTPUT_DIR_SPECIFIC, \"grid_views\", f\"{class_name}_{img_id}_grid.png\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(grid_path_specific, dpi=150, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # Store result for summary\n",
        "                specific_results.append({\n",
        "                    'path': img_path,\n",
        "                    'class': class_name,\n",
        "                    'filename': filename,\n",
        "                    'img_id': img_id,\n",
        "                    'score': max_score,\n",
        "                    'prediction': prediction,\n",
        "                    'has_contours': len(contours) > 0,\n",
        "                    'grid_path': grid_path_specific # Use the path from the specific output\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing specific test image {img_path}: {e}\")\n",
        "                traceback.print_exc() # Use traceback.print_exc() here\n",
        "\n",
        "    # Create a separate summary report for specific tests\n",
        "    try:\n",
        "        print(\"Creating specific test summary report...\")\n",
        "        report_path_specific = os.path.join(OUTPUT_DIR_SPECIFIC, \"specific_test_summary_report.html\")\n",
        "        with open(report_path_specific, 'w') as f:\n",
        "            f.write(\"<html><head>\")\n",
        "            f.write(\"<style>\")\n",
        "            f.write(\"body { font-family: Arial, sans-serif; margin: 20px; }\")\n",
        "            f.write(\"h1 { color: #333; }\")\n",
        "            f.write(\"table { border-collapse: collapse; width: 100%; }\")\n",
        "            f.write(\"th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\")\n",
        "            f.write(\"th { background-color: #f2f2f2; }\")\n",
        "            f.write(\"tr:nth-child(even) { background-color: #f9f9f9; }\")\n",
        "            f.write(\"img { max-width: 800px; border: 1px solid #ddd; }\") # Smaller image size for specific tests\n",
        "            f.write(\"</style>\")\n",
        "            f.write(\"</head><body>\")\n",
        "            f.write(\"<h1>Specific Anomaly Detection Test Results</h1>\")\n",
        "\n",
        "            # Summary statistics\n",
        "            good_samples_specific = [r for r in specific_results if r['class'] == 'good']\n",
        "            defect_samples_specific = [r for r in specific_results if r['class'] != 'good']\n",
        "\n",
        "            f.write(f\"<p>Total samples tested: {len(specific_results)}</p>\")\n",
        "            f.write(f\"<p>Good samples tested: {len(good_samples_specific)}</p>\")\n",
        "            f.write(f\"<p>Defect samples tested: {len(defect_samples_specific)}</p>\")\n",
        "\n",
        "            # Table of results\n",
        "            f.write(\"<h2>Results Table</h2>\")\n",
        "            f.write(\"<table>\")\n",
        "            f.write(\"<tr><th>ID</th><th>Class</th><th>Score</th><th>Prediction</th><th>Grid View</th></tr>\")\n",
        "\n",
        "            for r in sorted(specific_results, key=lambda x: x['score'], reverse=True):\n",
        "                f.write(f\"<tr>\")\n",
        "                f.write(f\"<td>{r['img_id']}</td>\")\n",
        "                f.write(f\"<td>{r['class']}</td>\")\n",
        "                f.write(f\"<td>{r['score']:.4f}</td>\")\n",
        "                f.write(f\"<td>{r['prediction']}</td>\")\n",
        "                # Link to the grid view image\n",
        "                f.write(f\"<td><a href='{os.path.relpath(r['grid_path'], OUTPUT_DIR_SPECIFIC)}' target='_blank'>View Grid</a></td>\")\n",
        "                f.write(f\"</tr>\")\n",
        "\n",
        "            f.write(\"</table>\")\n",
        "\n",
        "            # Include the grid views directly below the table\n",
        "            f.write(\"<h2>Grid Views</h2>\")\n",
        "            for r in sorted(specific_results, key=lambda x: x['class']): # Sort by class for better grouping\n",
        "                 f.write(f\"<h3>{r['filename']} (Class: {r['class']}, Score: {r['score']:.4f})</h3>\")\n",
        "                 f.write(f\"<img src='{os.path.relpath(r['grid_path'], OUTPUT_DIR_SPECIFIC)}'>\")\n",
        "                 f.write(\"<hr>\")\n",
        "\n",
        "\n",
        "            f.write(\"</body></html>\")\n",
        "\n",
        "        print(f\"Specific test summary report created at: {report_path_specific}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating specific test summary report: {e}\")\n",
        "\n",
        "\n",
        "    print(f\"Specific test results saved to {OUTPUT_DIR_SPECIFIC}\")\n",
        "    print(\"Specific tests complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3gGvH6LY77N",
        "outputId": "a5f0b8ab-5f8d-4f95-bea0-356c3c934ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 5 good images from: /content/drive/MyDrive/wood_otsu/wood/test/good\n",
            "Testing 5 defect images from: /content/drive/MyDrive/wood_otsu/wood/test/defect\n",
            "Selected 5 good images.\n",
            "Selected 5 defect images.\n",
            "Loading models and quantiles...\n",
            "Successfully loaded final models\n",
            "Loading quantiles...\n",
            "Successfully loaded quantiles\n",
            "Loading training images for normalization (if needed)...\n",
            "WARNING: No training images found. Using dummy statistics.\n",
            "Starting inference on selected images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth mask not found for /content/drive/MyDrive/wood_otsu/wood/test/good/72.jpg. Creating blank mask.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  10%|â–ˆ         | 1/10 [00:00<00:04,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth mask not found for /content/drive/MyDrive/wood_otsu/wood/test/good/140.jpg. Creating blank mask.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  20%|â–ˆâ–ˆ        | 2/10 [00:01<00:04,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth mask not found for /content/drive/MyDrive/wood_otsu/wood/test/good/106.jpg. Creating blank mask.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:01<00:03,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth mask not found for /content/drive/MyDrive/wood_otsu/wood/test/good/117.jpg. Creating blank mask.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:02<00:03,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth mask not found for /content/drive/MyDrive/wood_otsu/wood/test/good/94.jpg. Creating blank mask.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:02<00:02,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/101900009_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:04<00:03,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100039_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:06<00:03,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100100026_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:07<00:02,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000018_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing selected test images:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:08<00:01,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ground truth mask at: /content/drive/MyDrive/wood_otsu/wood/ground_truth/defect/100000007_mask.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing selected test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating specific test summary report...\n",
            "Specific test summary report created at: /content/drive/MyDrive/EfficientAD_/result-specific_images/png/specific_test_summary_report.html\n",
            "Specific test results saved to /content/drive/MyDrive/EfficientAD_/result-specific_images/png\n",
            "Specific tests complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import tifffile\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve # Import roc_curve here as well for clarity\n",
        "\n",
        "# Fix for PyTorch 2.6+ pickle loading issue\n",
        "from torch.serialization import add_safe_globals\n",
        "add_safe_globals([\n",
        "    'torch.nn.modules.container.Sequential',\n",
        "    'torch.nn.modules.conv.Conv2d',\n",
        "    'torch.nn.modules.activation.ReLU',\n",
        "    'torch.nn.modules.pooling.AvgPool2d',\n",
        "    'torch.nn.modules.linear.Linear',\n",
        "    'torch.nn.modules.normalization.GroupNorm',\n",
        "    'torch.nn.modules.activation.GELU',\n",
        "    'torchvision.models.resnet.Bottleneck',\n",
        "    'collections.OrderedDict'\n",
        "])\n",
        "\n",
        "# Paths - UPDATE THESE TO MATCH YOUR ACTUAL DIRECTORIES\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/wood_otsu/wood/test\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/EfficientAD_/result-13MayÄ±s_/png\"\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Wood_Dataset/wood/train\"  # Path to training images\n",
        "GT_DIR = \"/content/drive/MyDrive/wood_otsu/wood/ground_truth\"  # Ground truth directory\n",
        "\n",
        "# Contour threshold for anomaly detection - Updated to 0.250\n",
        "CONTOUR_THRESHOLD = 0.0381\n",
        "# Print paths for verification\n",
        "print(f\"Dataset Path: {DATASET_PATH}\")\n",
        "print(f\"Model Path: {MODEL_PATH}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"Train Directory: {TRAIN_DIR}\")\n",
        "print(f\"Ground Truth Directory: {GT_DIR}\")\n",
        "print(f\"Contour Threshold: {CONTOUR_THRESHOLD}\")\n",
        "\n",
        "# Check if directories exist\n",
        "for path, name in [(DATASET_PATH, \"Dataset\"), (MODEL_PATH, \"Model\"), (TRAIN_DIR, \"Train\"), (GT_DIR, \"Ground Truth\")]:\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"WARNING: {name} path does not exist: {path}\")\n",
        "    else:\n",
        "        print(f\"{name} path exists: {path}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"grid_views\"), exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "on_gpu = torch.cuda.is_available()\n",
        "out_channels = 384\n",
        "image_size = 256\n",
        "\n",
        "# Default transform for testing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Helper functions for contour detection and border removal\n",
        "def filter_contours(contours, image_shape, min_area=100, border_margin=30):\n",
        "    \"\"\"Filter contours based on size and position\"\"\"\n",
        "    h, w = image_shape[:2]\n",
        "    filtered = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        x, y, cw, ch = cv2.boundingRect(cnt)\n",
        "        if area > min_area and border_margin < x < w-border_margin and border_margin < y < h-border_margin:\n",
        "            filtered.append(cnt)\n",
        "    return filtered\n",
        "\n",
        "def remove_border_artifacts(anomaly_map, border_px=30):\n",
        "    \"\"\"Remove border artifacts from anomaly map\"\"\"\n",
        "    h, w = anomaly_map.shape\n",
        "    mask = np.zeros_like(anomaly_map)\n",
        "    mask[border_px:h-border_px, border_px:w-border_px] = 1\n",
        "    return anomaly_map * mask\n",
        "\n",
        "print(\"Loading models and quantiles...\")\n",
        "try:\n",
        "    # Try loading with weights_only=False to handle PyTorch 2.6 changes\n",
        "    teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_final.pth'), weights_only=False)\n",
        "    student = torch.load(os.path.join(MODEL_PATH, 'student_final.pth'), weights_only=False)\n",
        "    autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_final.pth'), weights_only=False)\n",
        "    print(\"Successfully loaded final models\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading final models: {e}\")\n",
        "    try:\n",
        "        # Try temporary models\n",
        "        print(\"Trying temporary models...\")\n",
        "        teacher = torch.load(os.path.join(MODEL_PATH, 'teacher_tmp.pth'), weights_only=False)\n",
        "        student = torch.load(os.path.join(MODEL_PATH, 'student_tmp.pth'), weights_only=False)\n",
        "        autoencoder = torch.load(os.path.join(MODEL_PATH, 'autoencoder_tmp.pth'), weights_only=False)\n",
        "        print(\"Successfully loaded temporary models\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporary models: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load quantiles\n",
        "print(\"Loading quantiles...\")\n",
        "try:\n",
        "    quantiles = torch.load(os.path.join(MODEL_PATH, 'map_quantiles_final.pth'), weights_only=False)\n",
        "    q_st_start = torch.tensor(quantiles['q_st_start']) # Convert to tensor\n",
        "    q_st_end = torch.tensor(quantiles['q_st_end'])  # Convert to tensor\n",
        "    q_ae_start = torch.tensor(quantiles['q_ae_start'])  # Convert to tensor\n",
        "    q_ae_end = torch.tensor(quantiles['q_ae_end'])  # Convert to tensor\n",
        "    print(\"Successfully loaded quantiles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading quantiles: {e}\")\n",
        "    raise\n",
        "\n",
        "if on_gpu:\n",
        "    teacher = teacher.cuda()\n",
        "    student = student.cuda()\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    q_st_start = q_st_start.cuda()\n",
        "    q_st_end = q_st_end.cuda()\n",
        "    q_ae_start = q_ae_start.cuda()\n",
        "    q_ae_end = q_ae_end.cuda()\n",
        "\n",
        "# Set models to evaluation mode\n",
        "teacher.eval()\n",
        "student.eval()\n",
        "autoencoder.eval()\n",
        "\n",
        "# Load train data for computing teacher normalization\n",
        "print(\"Loading training images for normalization...\")\n",
        "\n",
        "# Try different patterns to locate training images\n",
        "train_patterns = [\n",
        "    os.path.join(TRAIN_DIR, \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"**\", \"*.png\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.jpg\"),\n",
        "    os.path.join(TRAIN_DIR, \"good\", \"*.png\")\n",
        "]\n",
        "\n",
        "train_images = []\n",
        "for pattern in train_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} images with pattern: {pattern}\")\n",
        "        train_images.extend(found_images)\n",
        "\n",
        "if not train_images:\n",
        "    print(\"ERROR: No training images found! Checking parent directories...\")\n",
        "    # Try to look in parent directories\n",
        "    parent_dir = os.path.dirname(TRAIN_DIR)\n",
        "    for pattern in [os.path.join(parent_dir, \"**\", \"*.jpg\"), os.path.join(parent_dir, \"**\", \"*.png\")]:\n",
        "        found_images = glob(pattern, recursive=True)\n",
        "        if found_images:\n",
        "            print(f\"Found {len(found_images)} images in parent directory with pattern: {pattern}\")\n",
        "            train_images.extend(found_images)\n",
        "\n",
        "# Print first few train images for verification\n",
        "if train_images:\n",
        "    print(f\"Total training images found: {len(train_images)}\")\n",
        "    print(\"First few training images:\")\n",
        "    for i in range(min(5, len(train_images))):\n",
        "        print(f\"  - {train_images[i]}\")\n",
        "else:\n",
        "    print(\"ERROR: Could not find any training images! Cannot proceed without training images.\")\n",
        "    # Instead of crashing, we'll use pre-computed statistics from the quantiles\n",
        "    print(\"Using pre-computed statistics from quantiles to continue...\")\n",
        "    # Create some dummy statistics that should work reasonably well\n",
        "    channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "    channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "    if on_gpu:\n",
        "        channel_mean = channel_mean.cuda()\n",
        "        channel_std = channel_std.cuda()\n",
        "\n",
        "# If we have training images, compute normalization statistics\n",
        "if train_images:\n",
        "    # If there are too many images, use just a subset for normalization\n",
        "    if len(train_images) > 50:\n",
        "        import random\n",
        "        train_images = random.sample(train_images, 50)\n",
        "        print(f\"Using random subset of 50 images for normalization\")\n",
        "\n",
        "    # Compute teacher normalization\n",
        "    print(\"Computing teacher normalization...\")\n",
        "    mean_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for img_path in tqdm(train_images):\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                image = transform(image).unsqueeze(0)\n",
        "                if on_gpu:\n",
        "                    image = image.cuda()\n",
        "                teacher_output = teacher(image)\n",
        "                mean_output = torch.mean(teacher_output, dim=[0, 2, 3])\n",
        "                mean_outputs.append(mean_output)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing training image {img_path}: {e}\")\n",
        "\n",
        "    if not mean_outputs:\n",
        "        print(\"ERROR: Failed to process any training images!\")\n",
        "        # Use dummy statistics\n",
        "        channel_mean = torch.zeros((1, out_channels, 1, 1))\n",
        "        channel_std = torch.ones((1, out_channels, 1, 1))\n",
        "        if on_gpu:\n",
        "            channel_mean = channel_mean.cuda()\n",
        "            channel_std = channel_std.cuda()\n",
        "    else:\n",
        "        channel_mean = torch.mean(torch.stack(mean_outputs), dim=0)\n",
        "        channel_mean = channel_mean[None, :, None, None]\n",
        "\n",
        "        mean_distances = []\n",
        "        with torch.no_grad():\n",
        "            for img_path in tqdm(train_images):\n",
        "                try:\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image = transform(image).unsqueeze(0)\n",
        "                    if on_gpu:\n",
        "                        image = image.cuda()\n",
        "                    teacher_output = teacher(image)\n",
        "                    distance = (teacher_output - channel_mean) ** 2\n",
        "                    mean_distance = torch.mean(distance, dim=[0, 2, 3])\n",
        "                    mean_distances.append(mean_distance)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing training image distance {img_path}: {e}\")\n",
        "\n",
        "        channel_var = torch.mean(torch.stack(mean_distances), dim=0)\n",
        "        channel_var = channel_var[None, :, None, None]\n",
        "        channel_std = torch.sqrt(channel_var)\n",
        "\n",
        "# Find all test images (both good and defect)\n",
        "print(\"Looking for test images...\")\n",
        "test_patterns = [\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"**\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"good\", \"*.png\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.jpg\"),\n",
        "    os.path.join(DATASET_PATH, \"defect\", \"*.png\")\n",
        "]\n",
        "\n",
        "test_images = []\n",
        "for pattern in test_patterns:\n",
        "    found_images = glob(pattern, recursive=True)\n",
        "    if found_images:\n",
        "        print(f\"Found {len(found_images)} test images with pattern: {pattern}\")\n",
        "        test_images.extend(found_images)\n",
        "\n",
        "# Remove duplicates\n",
        "test_images = list(set(test_images))\n",
        "\n",
        "if not test_images:\n",
        "    print(\"ERROR: No test images found!\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Found {len(test_images)} total test images.\")\n",
        "print(\"First few test images:\")\n",
        "for i in range(min(5, len(test_images))):\n",
        "    print(f\"  - {test_images[i]}\")\n",
        "\n",
        "# Run inference on all test images\n",
        "print(\"Starting inference...\")\n",
        "results = []\n",
        "y_true = [] # Initialize list to store true labels\n",
        "y_score = [] # Initialize list to store anomaly scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_path in tqdm(test_images, desc=\"Processing test images\"):\n",
        "        try:\n",
        "            # Extract class and filename\n",
        "            relative_path = os.path.relpath(img_path, DATASET_PATH)\n",
        "            parts = relative_path.split(os.sep)\n",
        "\n",
        "            if len(parts) > 1:\n",
        "                class_name = parts[0]\n",
        "                filename = parts[1]\n",
        "                # Determine true label (0 for good, 1 for defect)\n",
        "                true_label = 0 if class_name == 'good' else 1\n",
        "            else:\n",
        "                class_name = \"unknown\"\n",
        "                filename = parts[0]\n",
        "                true_label = -1 # Indicate unknown or unable to determine class\n",
        "\n",
        "            img_id = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Create output directory for this class\n",
        "            class_output_dir = os.path.join(OUTPUT_DIR, class_name)\n",
        "            os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            orig_width, orig_height = image.size\n",
        "            image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "            if on_gpu:\n",
        "                image_tensor = image_tensor.cuda()\n",
        "\n",
        "            # Run EfficientAD inference\n",
        "            teacher_output = teacher(image_tensor)\n",
        "            teacher_output = (teacher_output - channel_mean) / channel_std\n",
        "            student_output = student(image_tensor)\n",
        "            autoencoder_output = autoencoder(image_tensor)\n",
        "\n",
        "            map_st = torch.mean((teacher_output - student_output[:, :out_channels])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "            map_ae = torch.mean((autoencoder_output - student_output[:, out_channels:])**2,\n",
        "                               dim=1, keepdim=True)\n",
        "\n",
        "            # Apply quantile normalization\n",
        "            map_st = 0.1 * (map_st - q_st_start) / (q_st_end - q_st_start)\n",
        "            map_ae = 0.1 * (map_ae - q_ae_start) / (q_ae_end - q_ae_start)\n",
        "\n",
        "            # Combine maps\n",
        "            map_combined = 0.5 * map_st + 0.5 * map_ae\n",
        "\n",
        "            # Pad and resize back to original dimensions\n",
        "            map_combined = torch.nn.functional.pad(map_combined, (4, 4, 4, 4))\n",
        "            map_combined = torch.nn.functional.interpolate(\n",
        "                map_combined, (orig_height, orig_width), mode='bilinear')\n",
        "\n",
        "            # Convert to numpy\n",
        "            anomaly_map = map_combined[0, 0].cpu().numpy()\n",
        "\n",
        "            # Save as tiff file (for full precision)\n",
        "            tiff_path = os.path.join(class_output_dir, f\"{img_id}.tiff\")\n",
        "            tifffile.imwrite(tiff_path, anomaly_map)\n",
        "\n",
        "            # Load original image for overlay\n",
        "            orig_img = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "            # Remove border artifacts for scoring\n",
        "            anomaly_map_masked = remove_border_artifacts(anomaly_map.copy(), border_px=40)\n",
        "            max_score = np.max(anomaly_map_masked)\n",
        "\n",
        "            # Append true label and anomaly score to lists\n",
        "            if true_label != -1: # Only append if we could determine the true class\n",
        "                y_true.append(true_label)\n",
        "                y_score.append(max_score)\n",
        "\n",
        "            # Create overlay with contours\n",
        "            overlay_img = orig_img.copy()\n",
        "\n",
        "            # Create binary mask for contours\n",
        "            binary_mask = (anomaly_map_masked > CONTOUR_THRESHOLD).astype(np.uint8) * 255\n",
        "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            contours = filter_contours(contours, orig_img.shape, min_area=200, border_margin=40)\n",
        "\n",
        "            # Draw contours on overlay\n",
        "            overlay_img_cv = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(overlay_img_cv, contours, -1, (0, 0, 255), 2)  # Red contours\n",
        "\n",
        "            # Add prediction text\n",
        "            prediction = \"Anomali\" if max_score > CONTOUR_THRESHOLD else \"Normal\"\n",
        "            cv2.putText(overlay_img_cv, f\"Tahmin: {prediction} ({max_score:.3f})\",\n",
        "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "            # Convert back to RGB for matplotlib\n",
        "            overlay_img = cv2.cvtColor(overlay_img_cv, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Create heatmap visualization\n",
        "            tiff_norm = cv2.normalize(anomaly_map, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "            tiff_colored = cv2.applyColorMap(tiff_norm, cv2.COLORMAP_JET)\n",
        "            tiff_colored = cv2.cvtColor(tiff_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Try to load ground truth mask if available\n",
        "            gt_mask = None\n",
        "            potential_gt_paths = [\n",
        "                os.path.join(GT_DIR, class_name, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, class_name, f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, \"defect\", f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, \"ground_truth\", class_name, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, \"ground_truth\", class_name, f\"{img_id}_mask.jpg\"),\n",
        "                os.path.join(GT_DIR, f\"{img_id}_mask.png\"),\n",
        "                os.path.join(GT_DIR, f\"{img_id}_mask.jpg\")\n",
        "            ]\n",
        "\n",
        "            for gt_path in potential_gt_paths:\n",
        "                if os.path.exists(gt_path):\n",
        "                    gt_mask = np.array(Image.open(gt_path).convert('RGB'))\n",
        "                    # print(f\"Found ground truth mask at: {gt_path}\") # Comment out to reduce output noise\n",
        "                    break\n",
        "\n",
        "            # If ground truth mask not found, create a blank one (for good images or missing masks)\n",
        "            if gt_mask is None:\n",
        "                gt_mask = np.ones_like(orig_img) * 255\n",
        "                # Only center part is black if it's a good image\n",
        "                if class_name == 'good':\n",
        "                    cv2.rectangle(gt_mask, (0, 0), (orig_width, orig_height), (0, 0, 0), -1)\n",
        "                    gt_mask[10:-10, 10:-10] = [255, 255, 255]\n",
        "\n",
        "\n",
        "            # Create grid visualization (Original, Anomaly Map, Overlay, Ground Truth)\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Original Image\n",
        "            plt.subplot(1, 4, 1)\n",
        "            plt.imshow(orig_img)\n",
        "            plt.title(f\"{filename} ({class_name}) - Original\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Anomaly Map\n",
        "            plt.subplot(1, 4, 2)\n",
        "            plt.imshow(tiff_colored)\n",
        "            plt.title(f\"Anomaly Map (.tiff) - Max: {max_score:.3f}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Overlay\n",
        "            plt.subplot(1, 4, 3)\n",
        "            plt.imshow(overlay_img)\n",
        "            plt.title(f\"Overlay (Tahmin: {prediction})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Ground Truth\n",
        "            plt.subplot(1, 4, 4)\n",
        "            plt.imshow(gt_mask)\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Save grid visualization\n",
        "            grid_path = os.path.join(OUTPUT_DIR, \"grid_views\", f\"{class_name}_{img_id}_grid.png\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(grid_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # Store result for summary\n",
        "            results.append({\n",
        "                'path': img_path,\n",
        "                'class': class_name,\n",
        "                'filename': filename,\n",
        "                'img_id': img_id,\n",
        "                'score': max_score,\n",
        "                'prediction': prediction,\n",
        "                'has_contours': len(contours) > 0,\n",
        "                'grid_path': grid_path,\n",
        "                'true_label': true_label # Add true label to results\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing test image {img_path}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "print(f\"All anomaly maps and visualizations saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# Convert y_true and y_score to numpy arrays before calculating ROC curve\n",
        "y_true = np.array(y_true)\n",
        "y_score = np.array(y_score)\n",
        "\n",
        "# Calculate ROC curve\n",
        "try:\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    youden_index = tpr - fpr\n",
        "    best_threshold = thresholds[np.argmax(youden_index)]\n",
        "    print(f\"Calculated best threshold using Youden's J statistic: {best_threshold:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error calculating ROC curve or best threshold: {e}\")\n",
        "\n",
        "\n",
        "# Create summary report\n",
        "try:\n",
        "    print(\"Creating summary report...\")\n",
        "    report_path = os.path.join(OUTPUT_DIR, \"summary_report.html\")\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"<html><head>\")\n",
        "        f.write(\"<style>\")\n",
        "        f.write(\"body { font-family: Arial, sans-serif; margin: 20px; }\")\n",
        "        f.write(\"h1 { color: #333; }\")\n",
        "        f.write(\"table { border-collapse: collapse; width: 100%; }\")\n",
        "        f.write(\"th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\")\n",
        "        f.write(\"th { background-color: #f2f2f2; }\")\n",
        "        f.write(\"tr:nth-child(even) { background-color: #f9f9f9; }\")\n",
        "        f.write(\"img { max-width: 1000px; border: 1px solid #ddd; }\")\n",
        "        f.write(\"</style>\")\n",
        "        f.write(\"</head><body>\")\n",
        "        f.write(\"<h1>Anomaly Detection Results</h1>\")\n",
        "\n",
        "        # Summary statistics\n",
        "        good_samples = [r for r in results if r['class'] == 'good']\n",
        "        defect_samples = [r for r in results if r['class'] != 'good']\n",
        "\n",
        "        f.write(f\"<p>Total samples: {len(results)}</p>\")\n",
        "        f.write(f\"<p>Good samples: {len(good_samples)}</p>\")\n",
        "        f.write(f\"<p>Defect samples: {len(defect_samples)}</p>\")\n",
        "        if 'best_threshold' in locals(): # Check if best_threshold was calculated\n",
        "             f.write(f\"<p>Calculated Best Threshold (Youden's J): {best_threshold:.4f}</p>\")\n",
        "\n",
        "\n",
        "        # Table of results\n",
        "        f.write(\"<h2>Results Table</h2>\")\n",
        "        f.write(\"<table>\")\n",
        "        f.write(\"<tr><th>ID</th><th>Class</th><th>Score</th><th>Prediction</th><th>True Label</th></tr>\") # Added True Label column\n",
        "\n",
        "        for r in sorted(results, key=lambda x: x['score'], reverse=True):\n",
        "            f.write(f\"<tr>\")\n",
        "            f.write(f\"<td>{r['img_id']}</td>\")\n",
        "            f.write(f\"<td>{r['class']}</td>\")\n",
        "            f.write(f\"<td>{r['score']:.4f}</td>\")\n",
        "            f.write(f\"<td>{r['prediction']}</td>\")\n",
        "            f.write(f\"<td>{r['true_label']}</td>\") # Display true label\n",
        "            f.write(f\"</tr>\")\n",
        "\n",
        "        f.write(\"</table>\")\n",
        "\n",
        "        # Images\n",
        "        f.write(\"<h2>Top Anomalies</h2>\")\n",
        "\n",
        "        # Sort by score and take top 20\n",
        "        top_anomalies = sorted(results, key=lambda x: x['score'], reverse=True)[:20]\n",
        "\n",
        "        for r in top_anomalies:\n",
        "            f.write(f\"<h3>{r['filename']} (Class: {r['class']}, Score: {r['score']:.4f})</h3>\")\n",
        "            f.write(f\"<img src='{os.path.relpath(r['grid_path'], OUTPUT_DIR)}'>\")\n",
        "            f.write(\"<hr>\")\n",
        "\n",
        "        f.write(\"</body></html>\")\n",
        "\n",
        "    print(f\"Summary report created at: {report_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating summary report: {e}\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUJmzArcZtlt",
        "outputId": "ebdbe4a9-d9d0-46e0-86bc-6386731e6d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Path: /content/drive/MyDrive/wood_otsu/wood/test\n",
            "Model Path: /content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\n",
            "Output Directory: /content/drive/MyDrive/EfficientAD_/result-13MayÄ±s_/png\n",
            "Train Directory: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Ground Truth Directory: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Contour Threshold: 0.0381\n",
            "Dataset path exists: /content/drive/MyDrive/wood_otsu/wood/test\n",
            "Model path exists: /content/drive/MyDrive/EfficientAD_/Deneme 2 AgÌ†Ä±rlÄ±klar/trainings/mvtec_ad/wood\n",
            "WARNING: Train path does not exist: /content/drive/MyDrive/Wood_Dataset/wood/train\n",
            "Ground Truth path exists: /content/drive/MyDrive/wood_otsu/wood/ground_truth\n",
            "Loading models and quantiles...\n",
            "Successfully loaded final models\n",
            "Loading quantiles...\n",
            "Successfully loaded quantiles\n",
            "Loading training images for normalization...\n",
            "ERROR: No training images found! Checking parent directories...\n",
            "ERROR: Could not find any training images! Cannot proceed without training images.\n",
            "Using pre-computed statistics from quantiles to continue...\n",
            "Looking for test images...\n",
            "Found 140 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/**/*.jpg\n",
            "Found 70 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/good/*.jpg\n",
            "Found 70 test images with pattern: /content/drive/MyDrive/wood_otsu/wood/test/defect/*.jpg\n",
            "Found 140 total test images.\n",
            "First few test images:\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/111.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/defect/101900001.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/84.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/110.jpg\n",
            "  - /content/drive/MyDrive/wood_otsu/wood/test/good/123.jpg\n",
            "Starting inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [03:08<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All anomaly maps and visualizations saved to /content/drive/MyDrive/EfficientAD_/result-13MayÄ±s_/png\n",
            "Calculated best threshold using Youden's J statistic: 0.0381\n",
            "Creating summary report...\n",
            "Summary report created at: /content/drive/MyDrive/EfficientAD_/result-13MayÄ±s_/png/summary_report.html\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}