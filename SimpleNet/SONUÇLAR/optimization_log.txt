====== Hiperparametre Optimizasyonu BaÅŸlatÄ±lÄ±yor (2025-05-20 17:49:23) ======
Quantile DeÄŸeri: 0.56
ğŸ” GÃ¶rÃ¼ntÃ¼ler iÅŸleniyor ve geliÅŸmiÅŸ Ã¶zellikler Ã§Ä±karÄ±lÄ±yor...
âœ… Toplam 141 gÃ¶rsel iÅŸlendi: 71 defect, 70 good
ğŸ” Toplam 20 Ã¶zellik kullanÄ±lÄ±yor:
  1. anomaly_ratio
  2. mean_intensity
  3. max_intensity
  4. topk_mean
  5. std_intensity
  6. p25
  7. p50
  8. p75
  9. p90
  10. iqr
  11. skewness
  12. kurtosis
  13. entropy
  14. area_ratio
  15. largest_area
  16. circularity
  17. extent
  18. num_contours
  19. avg_contour_size
  20. complexity
ğŸ“Š Ã‡Ä±karÄ±lan Ã¶zellikler kaydedildi: /content/eval_outputs/extracted_features.csv

=== GridSearchCV ile Hiperparametre Optimizasyonu ===

ğŸ” DecisionTree iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… DecisionTree en iyi parametreler: {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}
âœ… DecisionTree en iyi F1 skoru: 0.7216

ğŸ” RandomForest iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… RandomForest en iyi parametreler: {'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}
âœ… RandomForest en iyi F1 skoru: 0.7154

ğŸ” GradientBoosting iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… GradientBoosting en iyi parametreler: {'learning_rate': 0.2, 'max_depth': 4, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}
âœ… GradientBoosting en iyi F1 skoru: 0.7339

ğŸ” XGBoost iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… XGBoost en iyi parametreler: {'colsample_bytree': 1.0, 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1.0}
âœ… XGBoost en iyi F1 skoru: 0.7211

ğŸ” AdaBoost iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… AdaBoost en iyi parametreler: {'learning_rate': 0.1, 'n_estimators': 50}
âœ… AdaBoost en iyi F1 skoru: 0.6834

ğŸ” SVM iÃ§in hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...
âœ… SVM en iyi parametreler: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}
âœ… SVM en iyi F1 skoru: 0.6705

=== Optuna ile XGBoost Ä°leri Seviye Optimizasyon ===
â­ Optuna ile XGBoost en iyi parametreler: {'n_estimators': 412, 'max_depth': 4, 'learning_rate': 0.013316421551547044, 'subsample': 0.8041407563609158, 'colsample_bytree': 0.8250151514139565, 'gamma': 0.22527790693090494, 'min_child_weight': 2, 'reg_alpha': 0.9138197111563866, 'reg_lambda': 0.29003911844062036}
â­ Optuna ile XGBoost en iyi F1 skoru: 0.6981
âŒ Optuna XGBoost optimizasyonu sÄ±rasÄ±nda hata: 
Image export using the "kaleido" engine requires the kaleido package,
which can be installed using pip:
    $ pip install -U kaleido


=== En Ä°yi PerformanslÄ± Modelleri Tespit Ediliyor ===
ğŸ¥‡ 1. sÄ±rada GradientBoosting: F1 Skoru = 0.7339
ğŸ¥‡ 2. sÄ±rada DecisionTree: F1 Skoru = 0.7216
ğŸ¥‡ 3. sÄ±rada XGBoost: F1 Skoru = 0.7211

=== En Ä°yi Model: GradientBoosting (F1: 0.7339) ===

ğŸ“Š SÄ±nÄ±flandÄ±rma Raporu:
              precision    recall  f1-score   support

      Normal       0.97      0.96      0.96        70
      Defect       0.96      0.97      0.97        71

    accuracy                           0.96       141
   macro avg       0.96      0.96      0.96       141
weighted avg       0.96      0.96      0.96       141


ğŸ“Š En Ã¶nemli 10 Ã¶zellik:
  1. p75: 0.1733
  2. iqr: 0.1053
  3. anomaly_ratio: 0.0944
  4. entropy: 0.0832
  5. max_intensity: 0.0724
  6. std_intensity: 0.0713
  7. p50: 0.0688
  8. kurtosis: 0.0661
  9. mean_intensity: 0.0613
  10. area_ratio: 0.0548

âœ… TÃ¼m sonuÃ§lar '/content/eval_outputs' klasÃ¶rÃ¼ne kaydedildi.
ğŸ“Š Tahminler: /content/eval_outputs/predictions_GradientBoosting.csv
ğŸ“Š Confusion Matrix: /content/eval_outputs/conf_matrix_GradientBoosting.png
ğŸ“Š Ã–zet rapor: /content/eval_outputs/optimization_summary.txt
